{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "from module.Weight_tune import *\n",
    "from module.Reorg import *\n",
    "from module.Cram import *\n",
    "from module.Init import *\n",
    "from module.LTS import *\n",
    "from module.Data import *\n",
    "import datetime\n",
    "# from utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Learning mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Full step for the learning algorithm mechanism\n",
    "# NOTE\n",
    "# 1. model right before reorg always need to be acceptable model\n",
    "# 2. model after cram and reorg always need to be acceptable \n",
    "# 3. check for the above every time after cram and reorg\n",
    "# 4. the randomness: in cram find r\n",
    "#################################################################\n",
    "from module.Cram import *\n",
    "from module.Reorg import *\n",
    "from module.Weight_tune import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewLearningAlgorithm():\n",
    "    def __init__(self):\n",
    "        # LOG\n",
    "        file_create_time = str(datetime.datetime.now().date())\n",
    "        self.out_file = open(f\"log/{file_create_time}\" + '.txt', 'w')\n",
    "        write(self.out_file, f\"#######################################\")\n",
    "        write(self.out_file, str(datetime.datetime.now()))\n",
    "        write(self.out_file, f\"#######################################\")\n",
    "        # DATA\n",
    "        data = pd.read_csv(\"Copper_forecasting_data.csv\")\n",
    "        (\n",
    "            self.X_train, \n",
    "            self.y_train, \n",
    "            self.X_valid, \n",
    "            self.y_valid, \n",
    "            self.X_test, \n",
    "            self.y_test\n",
    "        ) = preprocess(data, new_learning_algorithm=False)\n",
    "        # TRAIN SET\n",
    "        self.input_dim = 18\n",
    "        self.dtype = torch.float64\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.lr_rate = .0001\n",
    "        self.lr_bound = 1e-8\n",
    "        self.epochs = 50\n",
    "        self.batch_size = 100\n",
    "        self.learning_goal = None\n",
    "        self.model = None\n",
    "        self.trainloader = None\n",
    "        self.validloader = None\n",
    "        self.testloader = None\n",
    "        # CONFIG\n",
    "        self.config_wt = None\n",
    "        self.config_cram = None\n",
    "        self.config_reorg  = None\n",
    "        \n",
    "    def data(self):\n",
    "        self.trainloader = loader(self.X_train, self.y_train)\n",
    "        self.validloader = loader(self.X_valid, self.y_valid)\n",
    "        self.testloader = loader(self.X_test, self.y_test)\n",
    "    \n",
    "    def init_model_config(self, thres=80):\n",
    "        print('INIT MODEL')\n",
    "        while True:\n",
    "            model = TwoLayerNet(18, 1, 1).to(device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "            train_losses = []\n",
    "            val_losses = []   \n",
    "            val_min_loss = 1000\n",
    "            for epoch in tqdm(range(100)):\n",
    "                train_loss = 0\n",
    "                # forward operation\n",
    "                model.train()\n",
    "                for X, y in self.trainloader:\n",
    "                    X, y = X.to(device), y.to(device)        \n",
    "                    optimizer.zero_grad()\n",
    "                    preds = model(X)\n",
    "                    loss = self.criterion(preds, y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    train_loss += loss.item()\n",
    "                train_losses.append(train_loss)\n",
    "                with torch.no_grad():\n",
    "                    val_loss = 0\n",
    "                    model.eval()\n",
    "                    for X, y in self.validloader:\n",
    "                        X, y = X.to(device), y.to(device)\n",
    "                        preds = model(X)\n",
    "                        loss = self.criterion(preds, y)\n",
    "                        val_loss += loss.item()\n",
    "                    val_losses.append(val_loss)\n",
    "                    if val_loss < val_min_loss:\n",
    "                        val_min_loss = val_loss\n",
    "                        self.model = model\n",
    "            if self.criterion(self.model(self.X_valid), self.y_valid) < .0002:\n",
    "                break\n",
    "        # CONFIG        \n",
    "        self.learning_goal = torch.tensor(\n",
    "            np.percentile(((self.model(self.X_train) - self.y_train)**2).cpu().detach().numpy(), thres))\\\n",
    "            .to(dtype = self.dtype).to(device)        \n",
    "        self.config_wt = {\n",
    "            \"epochs\": 10,\n",
    "            \"criterion\": self.criterion,        # loss function\n",
    "            \"lr_rate\": self.lr_rate,            # learning rate \n",
    "            \"lr_bound\": self.lr_bound,          # lower bound of learning rate \n",
    "            \"lr_goal\": self.learning_goal,      # if regular eps < eps_reg: accept the model\n",
    "        }\n",
    "        self.config_cram = {\n",
    "            \"lr_goal\": self.learning_goal, \n",
    "            \"s\": 0.001,                     # a small num in cram\n",
    "        }\n",
    "        self.config_reorg  = {\n",
    "            \"epochs\": 1,\n",
    "            \"criterion\": self.criterion,        # loss function\n",
    "            \"lr_rate\": 0.00001,            # learning rate \n",
    "            \"lr_bound\": self.lr_bound,          # lower bound of learning rate \n",
    "            \"lr_goal\": self.learning_goal,      # if regular eps < eps_reg: accept the model\n",
    "            \"print_reg\": False,            # print detail, eg. loss for each epoch, or not\n",
    "            \"print_w_tune\": False,         # print detail, eg. loss for each epoch, or not\n",
    "            \"validate_run\": False,         # validate the model, or not\n",
    "        }\n",
    "     \n",
    "    def train(self, print_ = False):\n",
    "        n, n_not_fit = 0, 100\n",
    "        \n",
    "        while n < len(self.X_train):\n",
    "            write(self.out_file, f\"---------> Start New lts\")\n",
    "            # Obtaining_LTS / selecting_LTS\n",
    "            self.trainloader, indices, _, _, n = \\\n",
    "                lts(self.model, self.X_train, self.y_train, self.learning_goal, n_not_fit, self.out_file)\n",
    "            acceptable, eps_sqaure, _ = acceptable_eps_ypred(self.trainloader, self.model, self.learning_goal)\n",
    "            torch.save(self.model, \"unacceptable/selecting.pth\")\n",
    "            if acceptable:\n",
    "                continue\n",
    "\n",
    "            # Weight-Tune\n",
    "            write(self.out_file, f\"---------> Start WEIGHT TUNE\\n\")\n",
    "            acceptable, self.model, train_loss_list, test_loss_list = \\\n",
    "                module_weight_EU_LG_UA(self.model, self.trainloader, self.validloader, self.out_file, **self.config_wt)\n",
    "            \n",
    "            if acceptable:\n",
    "                write(self.out_file, \"---------> Start REORG (accpetable wt)\\n\")   \n",
    "                write(self.out_file, f\"model after wt: {self.model}\") if print else None\n",
    "                pre_module = \"wt\"\n",
    "            else:\n",
    "                write(self.out_file, \"---------> Start CRAM and REORG (unacceptable wt)\\n\")\n",
    "\n",
    "                # Cram\n",
    "                self.model = torch.load(\"unacceptable/selecting.pth\")    \n",
    "                acceptable, eps_square_before, _ = acceptable_eps_ypred(self.trainloader, self.model, self.learning_goal)\n",
    "                cram = cramming(self.model, self.X_train[indices], self.y_train[indices], self.out_file, **self.config_cram)            \n",
    "                cram.cram() \n",
    "                self.model = cram.model\n",
    "                acceptable, eps_square, _ = acceptable_eps_ypred(self.trainloader, self.model, self.learning_goal)\n",
    "                write(self.out_file, f\"eps_sqaure (last 10) after cram: {eps_square[-10:].reshape(-1)}\\n\", False)\n",
    "                write(self.out_file, f\"eps_square (last 10) before cram: {eps_square_before[-10:].reshape(-1)}\\n\", False)\n",
    "                assert acceptable, f\"weird cram, max eps_square{max(eps_sqaure)}\"\n",
    "                pre_module = \"Cram\"\n",
    "                \n",
    "            # Reorganising\n",
    "            \"\"\"\n",
    "            reorg = reorganising(pre_module, self.trainloader, self.testloader, self.out_file, **self.config_reorg)\n",
    "            reorg.reorganising()\n",
    "            self.model = reorg.model \n",
    "            acceptable, _, _ = acceptable_eps_ypred(self.trainloader, self.model, self.learning_goal)\n",
    "            assert acceptable, f\"weird reorg, max eps_square{max(eps_sqaure)}\"\n",
    "            \"\"\"\n",
    "            \n",
    "        torch.save(self.model, 'result/model.pth')\n",
    "        \n",
    "    def evaluate_test(self):\n",
    "        print(f\"train loss: {self.criterion(self.model(self.X_train), self.y_train)}\")\n",
    "        print(f'train residual max {torch.max((self.model(self.X_train) - self.y_train)**2)}')\n",
    "        print(f\"test loss: {self.criterion(self.model(self.X_test), self.y_test)}\")\n",
    "        print(f'test residual max {torch.max((self.model(self.X_test) - self.y_test)**2)}')\n",
    "    \n",
    "    def evaluate_valid(self):\n",
    "        validate_loss = self.criterion(self.model(self.X_valid), self.y_valid)\n",
    "        validate_residual_max = torch.max((self.model(self.X_valid) - self.y_valid)**2)\n",
    "        print(f\"valid loss: {validate_loss}\")\n",
    "        print(f'valid residual max {validate_residual_max}')\n",
    "        \"\"\"\n",
    "        print(f\"train loss: {\n",
    "            self.criterion(self.model(self.X_train), self.y_train)}\")\n",
    "        print(f'train residual max {torch.max((self.model(self.X_train) - self.y_train)**2)}')\n",
    "        \"\"\"\n",
    "        return validate_loss, validate_residual_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threses = [50, 80, 90, 95, 98]\n",
    "min_of_val_res_max = 100\n",
    "nla = NewLearningAlgorithm()\n",
    "nla.data()\n",
    "for thres in threses:\n",
    "    print('=======================================')\n",
    "    print('[NEW TRIAL]')\n",
    "    nla.init_model_config(thres)\n",
    "    print('=====')\n",
    "    print('Test Time')\n",
    "    nla.evaluate_test()\n",
    "    print('=====')\n",
    "    nla.train(thres)\n",
    "    val_loss, val_res_max = nla.evaluate_valid()\n",
    "    print('=====')\n",
    "    print('Test Time')\n",
    "    nla.evaluate_test()\n",
    "    print('=====')\n",
    "    if min_of_val_res_max > val_res_max:\n",
    "        print('BEST THRES!!!!!!!!!!!!!!!!!!!!')\n",
    "        min_of_val_res_max = val_res_max\n",
    "        best_thres = thres\n",
    "        best_model = nla.model\n",
    "        best_nla = nla\n",
    "        torch.save(best_model, 'result/best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.0002499454853479313\n",
      "train residual max 0.0037278311435575253\n",
      "test loss: 0.0012928470864951036\n",
      "test residual max 0.007970902348198992\n"
     ]
    }
   ],
   "source": [
    "best_nla.evaluate_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threses = [50, 80, 90, 95, 98]\n",
    "min_of_val_res_max = 100\n",
    "nla = NewLearningAlgorithm()\n",
    "nla.data()\n",
    "for thres in threses:\n",
    "    print('=======================================')\n",
    "    print('[NEW TRIAL]')\n",
    "    nla.init_model_config(thres)\n",
    "    if min_of_val_res_max > val_res_max:\n",
    "        print('BEST THRES!!!!!!!!!!!!!!!!!!!!')\n",
    "        min_of_val_res_max = val_res_max\n",
    "        best_thres = thres\n",
    "        best_model = nla.model\n",
    "        best_nla = nla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.0007156500724556761\n",
      "train residual max 0.012368260482324613\n",
      "test loss: 0.0009486841600028468\n",
      "test residual max 0.006449873076933106\n"
     ]
    }
   ],
   "source": [
    "best_nla.evaluate_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "263930470851f494f0ed2879c35b57985588df20f9e529b86e97dd5eb9ddc466"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
