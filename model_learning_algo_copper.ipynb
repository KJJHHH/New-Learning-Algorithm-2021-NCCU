{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from module.Weight_tune import *\n",
    "from module.Reorg import *\n",
    "from module.Cram import *\n",
    "from module.Init import *\n",
    "from module.LTS import *\n",
    "from module.Data import *\n",
    "import datetime\n",
    "# from utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Learning mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [00:02<00:01, 23.66it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     27\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 28\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# INITIALISE MODEL WITH TRAINING TWO LAYER NET\n",
    "import torch\n",
    "data = pd.read_csv(\"Copper_forecasting_data.csv\")\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = preprocess(data, new_learning_algorithm=False)\n",
    "input_dim = X_train.shape[1]\n",
    "train_loader = loader(X_train, y_train)\n",
    "valid_loader = loader(X_valid, y_valid)\n",
    "test_loader = loader(X_test, y_test)\n",
    "\n",
    "while True:\n",
    "    model = TwoLayerNet(18, 1, 1).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    train_losses = []\n",
    "    val_losses = []   \n",
    "    val_min_loss = 1000\n",
    "    for epoch in tqdm(range(100)):\n",
    "        train_loss = 0\n",
    "        # forward operation\n",
    "        model.train()\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(device), y.to(device)        \n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X)\n",
    "            loss = criterion(preds, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_losses.append(train_loss)\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            model.eval()\n",
    "            for X, y in valid_loader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                preds = model(X)\n",
    "                loss = criterion(preds, y)\n",
    "                val_loss += loss.item()\n",
    "            val_losses.append(val_loss)\n",
    "            if val_loss < val_min_loss:\n",
    "                val_min_loss = val_loss\n",
    "                best_model = model\n",
    "    print(f\"train loss: {criterion(best_model(X_train), y_train)}\")\n",
    "    print(f\"val loss: {criterion(best_model(X_valid), y_valid)}\")\n",
    "    if criterion(best_model(X_valid), y_valid) < .001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001186559583611701"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Copper_forecasting_data.csv\")\n",
    "X_train, y_train, X_test, y_test = preprocess(data)\n",
    "input_dim = X_train.shape[1]\n",
    "train_loader = loader(X_train, y_train)\n",
    "test_loader = loader(X_test, y_test)\n",
    "X_train.device, y_train.device\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "np.percentile(((best_model(X_train) - y_train)**2).cpu().detach().numpy(), 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning goal: max eps < learning goal\n",
    "dtype = torch.float64\n",
    "criterion = nn.MSELoss()\n",
    "model = best_model\n",
    "learning_goal = \\\n",
    "    torch.tensor(np.percentile(((model(X_train) - y_train)**2).cpu().detach().numpy(), 95))\\\n",
    "    .to(dtype = dtype).to(device)\n",
    "lr_rate = .0001\n",
    "lr_bound = 1e-8\n",
    "epochs = 50\n",
    "batch_size = 100\n",
    "\"\"\"\n",
    "# Note\n",
    "1. hidden dim should check the previous model. dynamically change\n",
    "2. For lr_rate, lr bound, lr goal (eps bound) are all the same fro eahc module\n",
    "\"\"\"\n",
    "\n",
    "# config for weight tune\n",
    "config_wt = {\n",
    "    \"epochs\": 10,\n",
    "    \"criterion\": criterion,        # loss function\n",
    "    \"lr_rate\": lr_rate,            # learning rate \n",
    "    \"lr_bound\": lr_bound,          # lower bound of learning rate \n",
    "    \"lr_goal\": learning_goal,      # if regular eps < eps_reg: accept the model\n",
    "}\n",
    "\n",
    "# config for cram\n",
    "config_cram = {\n",
    "    \"lr_goal\": learning_goal, \n",
    "    \"s\": 0.001,                     # a small num in cram\n",
    "}\n",
    "\n",
    "# config for reorganise\n",
    "config_reorg  = {\n",
    "    \"epochs\": 10,\n",
    "    \"criterion\": criterion,        # loss function\n",
    "    \"lr_rate\": 0.00001,            # learning rate \n",
    "    \"lr_bound\": lr_bound,          # lower bound of learning rate \n",
    "    \"lr_goal\": learning_goal,      # if regular eps < eps_reg: accept the model\n",
    "    \"print_reg\": False,            # print detail, eg. loss for each epoch, or not\n",
    "    \"print_w_tune\": False,         # print detail, eg. loss for each epoch, or not\n",
    "    \"validate_run\": False,         # validate the model, or not\n",
    "}\n",
    "# NOTE \n",
    "# 1. for leaning goals, if first using weightune and no LTS or otherthings, \n",
    "# 13 for getting acceptable wt | 10 not acceptable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L11 p9, third learning mechanism\\\n",
    "`Check: should y add .reshape(-1, 1)? since some warning shows when testing the final model`\\\n",
    "`Check validate section`    \n",
    "`Check not using test data to train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([423, 18])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Full step for the learning algorithm mechanism\n",
    "# NOTE\n",
    "# 1. model right before reorg always need to be acceptable model\n",
    "# 2. model after cram and reorg always need to be acceptable \n",
    "# 3. check for the above every time after cram and reorg\n",
    "# 4. the randomness: in cram find r\n",
    "#################################################################\n",
    "from module.Cram import *\n",
    "from module.Reorg import *\n",
    "from module.Weight_tune import *\n",
    "\n",
    "# 0. Open log file\n",
    "file_create_time = str(datetime.datetime.now().date())\n",
    "out_file = open(f\"log/{file_create_time}\" + '.txt', 'a')\n",
    "write(out_file, f\"#######################################\")\n",
    "write(out_file, str(datetime.datetime.now()))\n",
    "write(out_file, f\"#######################################\")\n",
    "\n",
    "# 1. initializing_1_ReLU_LR | L11 p2\n",
    "\"\"\"\n",
    "model = init_model(X_train, y_train)\n",
    "\"\"\"\n",
    "n = 0\n",
    "n_not_fit = 1\n",
    "\n",
    "while n < len(X_train):\n",
    "    write(out_file, f\"######################################\")\n",
    "    write(out_file, f\"---------> Start New lts\")\n",
    "    # 2. obtaining_LTS / selecting_LTS\n",
    "    train_loader, indices, X_train_lts, y_train_lts, n = \\\n",
    "        lts(model, X_train, y_train, learning_goal, n_not_fit, out_file)\n",
    "\n",
    "    # 3. check learning goal\n",
    "    acceptable, eps_sqaure, y_pred = acceptable_eps_ypred(train_loader, model, learning_goal)\n",
    "    if acceptable:\n",
    "        continue\n",
    "\n",
    "    torch.save(model, \"unacceptable/selecting.pth\")\n",
    "\n",
    "    # store model in acceptable/wt.pth if acceptable\n",
    "    # store model in unacceptable/wt.pth if not acceptable\n",
    "    write(out_file, f\"---------> Start module_EU_LG_UA Epoch\")\n",
    "    acceptable, model, train_loss_list, test_loss_list = \\\n",
    "        module_weight_EU_LG_UA(model, train_loader, test_loader, out_file, **config_wt)\n",
    "    \n",
    "    if acceptable:\n",
    "        write(out_file, \"---------> Start REORG with accpetable wt\")   \n",
    "        write(out_file, f\"model after wt: {model}\")\n",
    "        \n",
    "        # load model in acceptable/wt.pth if wt acceptable\n",
    "        pre_module = \"wt\"\n",
    "        reorg = reorganising(pre_module, train_loader, test_loader, out_file, **config_reorg)\n",
    "        reorg.reorganising()\n",
    "        model = reorg.model\n",
    "        write(out_file, f\"model after reorg: {model}\")\n",
    "\n",
    "    else:\n",
    "        write(out_file, \"---------> Start CRAM and REORG with unacceptable wt //////////\")\n",
    "\n",
    "        # load model before wt: unacceptable/selecting.pth if unacceptable after wt\n",
    "        model = torch.load(\"unacceptable/selecting.pth\")    \n",
    "        acceptable, eps_square, y_pred = acceptable_eps_ypred(train_loader, model, learning_goal)\n",
    "        # write(out_file, f\"model after wt: {model}\")\n",
    "        write(out_file, f\"eps_square (last 10) before cram: {eps_square[-10:].reshape(-1)}\")\n",
    "\n",
    "        # load model in unacceptable/wt.pth if wt not acceptable       \n",
    "        # store acceptable cram in acceptable/cram.pth \n",
    "        cram = cramming(model, X_train[indices], y_train[indices], out_file, **config_cram)            \n",
    "        cram.cram() \n",
    "        model = cram.model\n",
    "        acceptable, eps_square, y_pred = acceptable_eps_ypred(train_loader, model, learning_goal)\n",
    "        # bwrite(out_file, f\"model after cram: {model}\")  \n",
    "        write(out_file, f\"eps_sqaure (last 10) after cram: {eps_square[-10:].reshape(-1)}\")\n",
    "        assert acceptable, \"weird cram\"\n",
    "\n",
    "        # load model in acceptable/cram.pth if cram acceptable\n",
    "        # store acceptable cram in acceptable/cram.pth\n",
    "        pre_module = \"Cram\"\n",
    "        reorg = reorganising(pre_module, train_loader, test_loader, out_file, **config_reorg)\n",
    "        reorg.reorganising()\n",
    "        if reorg.model == \"fail regularise\" or reorg.model == \"no acceptable slfn\":\n",
    "            write(out_file, \"weird reorg\")\n",
    "\n",
    "        model = reorg.model\n",
    "        write(out_file, f\"model after reorg: {model}\")\n",
    "\n",
    "        # check if acceptable\n",
    "        acceptable, eps_square, y_pred = acceptable_eps_ypred(train_loader, model, learning_goal)\n",
    "        assert acceptable, \"weird reorg\"\n",
    "            \n",
    "            \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "torch.save(model, 'result/model.pth')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.00016118382242293717\n",
      "test loss: 0.0004964259004033544\n",
      "train residual max 0.0011850889065952751\n",
      "test residual max 0.005007919463483838\n"
     ]
    }
   ],
   "source": [
    "model_path = 'result/model.pth'\n",
    "model = torch.load(model_path)\n",
    "print(f\"train loss: {criterion(model(X_train), y_train)}\")\n",
    "print(f\"test loss: {criterion(model(X_test), y_test)}\")\n",
    "print(f'train residual max {torch.max((model(X_train) - y_train)**2)}')\n",
    "print(f'test residual max {torch.max((model(X_test) - y_test)**2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([380, 18]),\n",
       " torch.Size([380, 1]),\n",
       " torch.Size([43, 18]),\n",
       " torch.Size([43, 1]),\n",
       " torch.Size([47, 18]),\n",
       " torch.Size([47, 1]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Copper_forecasting_data.csv\")\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = preprocess(data, new_learning_algorithm=False)\n",
    "input_dim = X_train.shape[1]\n",
    "train_loader = loader(X_train, y_train)\n",
    "valid_loader = loader(X_valid, y_valid)\n",
    "test_loader = loader(X_test, y_test)\n",
    "X_train.device, y_train.device\n",
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "# Define the neural network class\n",
    "class MultiLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MultiLayerNet, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Append layers to the list\n",
    "        for i, hid in enumerate(hidden_size):\n",
    "            self.layers.append(nn.Linear(input_size, hid).to(dtype=dtype))  # Append linear layer\n",
    "            if i+1 != len(hidden_size):  # Skip last layer\n",
    "                self.layers.append(nn.ReLU())  # Append ReLU activation after each linear layer\n",
    "            input_size = hid  # Update input dimension for the next layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "def train_multiple_layer(hidden_sizes):\n",
    "    while True:\n",
    "        model = MultiLayerNet(18, hidden_sizes, 1).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        train_losses = []\n",
    "        val_losses = []   \n",
    "        val_min_loss = 1000\n",
    "        for epoch in tqdm(range(500)):\n",
    "            train_loss = 0\n",
    "            # forward operation\n",
    "            for X, y in train_loader:\n",
    "                X, y = X.to(device), y.to(device)      \n",
    "                optimizer.zero_grad()\n",
    "                preds = model(X)\n",
    "                loss = criterion(preds, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            train_losses.append(train_loss)\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for X, y in valid_loader:\n",
    "                    X, y = X.to(device), y.to(device)\n",
    "                    preds = model(X)\n",
    "                    loss = criterion(preds, y)\n",
    "                    val_loss += loss.item()\n",
    "                val_losses.append(val_loss)\n",
    "                if val_loss < val_min_loss:\n",
    "                    val_min_loss = val_loss\n",
    "                    best_model = model\n",
    "        print(f\"train loss: {criterion(best_model(X_train), y_train)}\")\n",
    "        print(f\"test loss: {criterion(best_model(X_test), y_test)}\")\n",
    "        if criterion(best_model(X_valid), y_valid) < .001:\n",
    "            break\n",
    "    print('Final result ----------------')\n",
    "    print(f\"train loss: {criterion(best_model(X_train), y_train)}\")\n",
    "    print(f\"test loss: {criterion(best_model(X_test), y_test)}\")\n",
    "    print(f'train residual max {torch.max((best_model(X_train) - y_train)**2)}')\n",
    "    print(f'test residual max {torch.max((best_model(X_test) - y_test)**2)}')\n",
    "    plt.plot(train_losses)\n",
    "    plt.plot(val_losses)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_multiple_layer([100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_multiple_layer([100, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Four layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 328/500 [00:13<00:06, 26.25it/s]"
     ]
    }
   ],
   "source": [
    "train_multiple_layer([100, 200, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Five layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_multiple_layer([100, 200, 200, 100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "263930470851f494f0ed2879c35b57985588df20f9e529b86e97dd5eb9ddc466"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
