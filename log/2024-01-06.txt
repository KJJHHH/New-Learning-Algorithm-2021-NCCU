################################## New lts #######################################
##########################################################################################################################################################################################################################################2024-01-06 09:34:01.731741############################################################################################################################################################################################################################################# New lts ###
Total obtaining n: 328
obtaining n over lr goal: 0
Total select n: 333
select n over lr goal: 5
##########################################################################################################################################################################################################################################2024-01-06 09:34:51.787995############################################################################################################################################################################################################################################# New lts ###
Total obtaining n: 328
obtaining n over lr goal: 0
Total select n: 333
select n over lr goal: 5
##########################################################################################################################################################################################################################################2024-01-06 09:36:26.139187############################################################################################################################################################################################################################################# New lts ###
Total obtaining n: 328
obtaining n over lr goal: 0
Total select n: 333
select n over lr goal: 5
//////////// Start module_EU_LG_UA Epoch ///////////
Finish epoch. non acceptable module at max eps tensor([9.8415], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
////////// Start CRAM and REORG with unacceptable wt //////////
model after wt: TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=1, bias=True)
  (layer_out): Linear(in_features=1, out_features=1, bias=True)
  (relu): ReLU()
)
eps_square (last 10) before cram: tensor([[6.9044],
        [7.2054],
        [7.2848],
        [7.3423],
        [7.3833],
        [7.5202],
        [7.6095],
        [7.6147],
        [7.6644],
        [7.6826]], device='cuda:0', dtype=torch.float64,
       grad_fn=<SliceBackward0>)
cramming sample 328th |0.00% total of 5
##########################################################################################################################################################################################################################################2024-01-06 09:39:09.906234############################################################################################################################################################################################################################################# New lts ###
Total obtaining n: 328
obtaining n over lr goal: 0
Total select n: 333
select n over lr goal: 5
//////////// Start module_EU_LG_UA Epoch ///////////
Finish epoch. non acceptable module at max eps tensor([9.8415], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
////////// Start CRAM and REORG with unacceptable wt //////////
model after wt: TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=1, bias=True)
  (layer_out): Linear(in_features=1, out_features=1, bias=True)
  (relu): ReLU()
)
eps_square (last 10) before cram: tensor([[6.9044],
        [7.2054],
        [7.2848],
        [7.3423],
        [7.3833],
        [7.5202],
        [7.6095],
        [7.6147],
        [7.6644],
        [7.6826]], device='cuda:0', dtype=torch.float64,
       grad_fn=<SliceBackward0>)
##########################################################################################################################################################################################################################################2024-01-06 09:40:13.889339############################################################################################################################################################################################################################################# New lts ###
Total obtaining n: 328
obtaining n over lr goal: 0
Total select n: 333
select n over lr goal: 5
//////////// Start module_EU_LG_UA Epoch ///////////
Finish epoch. non acceptable module at max eps tensor([9.8415], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
////////// Start CRAM and REORG with unacceptable wt //////////
model after wt: TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=1, bias=True)
  (layer_out): Linear(in_features=1, out_features=1, bias=True)
  (relu): ReLU()
)
eps_square (last 10) before cram: tensor([[6.9044],
        [7.2054],
        [7.2848],
        [7.3423],
        [7.3833],
        [7.5202],
        [7.6095],
        [7.6147],
        [7.6644],
        [7.6826]], device='cuda:0', dtype=torch.float64,
       grad_fn=<SliceBackward0>)
cramming sample 328th |0.00% total of 5
cramming sample 329th |20.00% total of 5
cramming sample 330th |40.00% total of 5
cramming sample 331th |60.00% total of 5
cramming sample 332th |80.00% total of 5
model after cram: TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)
eps_sqaure (last 10) after cram: tensor([[6.9044e+00],
        [7.2054e+00],
        [7.2848e+00],
        [7.3423e+00],
        [7.3833e+00],
        [4.1442e-23],
        [1.3579e-24],
        [2.3381e-22],
        [5.8888e-25],
        [4.1717e-23]], device='cuda:0', dtype=torch.float64,
       grad_fn=<SliceBackward0>)
Acceptable SLFN exist in 'acceptable/cram.pth'.
=================== reorganising ===================
[ 0.00%] ------------> Checking nodes...
TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)
    --> Start regularising_EU_LG_UA
Try trim model: Copy model and delete nodes success
    --> Start module_EU_LG
[ 6.25%] ------------> Checking nodes...
TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)
    --> Start regularising_EU_LG_UA
Try trim model: Copy model and delete nodes success
    --> Start module_EU_LG
[ 12.50%] ------------> Checking nodes...
TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)
    --> Start regularising_EU_LG_UA
Try trim model: Copy model and delete nodes success
    --> Start module_EU_LG
[ 18.75%] ------------> Checking nodes...
TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)
    --> Start regularising_EU_LG_UA
Try trim model: Copy model and delete nodes success
    --> Start module_EU_LG
[ 25.00%] ------------> Checking nodes...
TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)
    --> Start regularising_EU_LG_UA
Try trim model: Copy model and delete nodes success
    --> Start module_EU_LG
[ 31.25%] ------------> Checking nodes...
TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)
    --> Start regularising_EU_LG_UA
Try trim model: Copy model and delete nodes success
    --> Start module_EU_LG
################################## New lts #######################################
#######################################
2024-01-06 17:01:14.412169#######################################
### New lts ###
Total obtaining n: 328
obtaining n over lr goal: 0
Total select n: 333
select n over lr goal: 5
//////////// Start module_EU_LG_UA Epoch ///////////
Finish epoch. non acceptable module at max eps tensor([9.8415], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
////////// Start CRAM and REORG with unacceptable wt //////////
model after wt: TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=1, bias=True)
  (layer_out): Linear(in_features=1, out_features=1, bias=True)
  (relu): ReLU()
)
eps_square (last 10) before cram: tensor([6.9044, 7.2054, 7.2848, 7.3423, 7.3833, 7.5202, 7.6095, 7.6147, 7.6644,
        7.6826], device='cuda:0', dtype=torch.float64, grad_fn=<ViewBackward0>)
cramming sample 328th | total of 5
cramming sample 329th | total of 5
cramming sample 330th | total of 5
cramming sample 331th | total of 5
cramming sample 332th | total of 5
model after cram: TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)
eps_sqaure (last 10) after cram: tensor([6.9044e+00, 7.2054e+00, 7.2848e+00, 7.3423e+00, 7.3833e+00, 8.0153e-25,
        3.7340e-23, 5.0926e-22, 3.8459e-24, 1.2892e-22], device='cuda:0',
       dtype=torch.float64, grad_fn=<ViewBackward0>)
Acceptable SLFN exist in 'acceptable/cram.pth'.
=================== reorganising ===================
---> Checking nodes...0/ 1600.00 
TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)
    --> Start regularising_EU_LG_UA
train_loss: 18678.05200432082
regular term: 2821.618678101536
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Try trim model: Copy model and delete nodes success
    --> Start module_EU_LG
train_loss: 2411.414319949641
max eps sqaure: tensor([3560.3749], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 2302.754679778946
max eps sqaure: tensor([3363.9304], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 2177.445304821833
max eps sqaure: tensor([3135.0178], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 2033.9348478325674
max eps sqaure: tensor([2869.3103], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 1871.1678649684711
max eps sqaure: tensor([2562.7073], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 1688.8663466501423
max eps sqaure: tensor([2211.6435], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 1489.0932814076598
max eps sqaure: tensor([2026.7584], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 1279.9609547811835
max eps sqaure: tensor([1890.2917], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 1076.4266361453226
max eps sqaure: tensor([1869.3264], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 900.2114285677211
max eps sqaure: tensor([1838.1098], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 766.6558334251714
max eps sqaure: tensor([1763.0000], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 662.549336739714
max eps sqaure: tensor([1603.9803], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 554.9496698756742
max eps sqaure: tensor([1404.8723], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 421.8851475688588
max eps sqaure: tensor([1019.5996], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 251.76599944593872
max eps sqaure: tensor([527.2651], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 93.42380209554226
max eps sqaure: tensor([380.7884], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 26.1960320660362
max eps sqaure: tensor([194.2677], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 20.74970747627766
max eps sqaure: tensor([123.4301], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 14.28354272559554
max eps sqaure: tensor([117.9690], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 9.847395475491274
max eps sqaure: tensor([74.2349], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 7.92474740307757
max eps sqaure: tensor([55.4588], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 6.626505034084034
max eps sqaure: tensor([41.6579], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 5.359991468892283
max eps sqaure: tensor([33.0473], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 4.566948604228542
max eps sqaure: tensor([27.8956], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 4.030997229230368
max eps sqaure: tensor([45.8462], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 3.8827120129888963
max eps sqaure: tensor([61.2775], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 3.5253674915043796
max eps sqaure: tensor([49.1624], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 3.2096064416380656
max eps sqaure: tensor([42.2702], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 3.1039229655229934
max eps sqaure: tensor([47.7016], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 3.309787955453091
max eps sqaure: tensor([55.6025], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Restore model and lr decrease
train_loss: 2.547144511931162
max eps sqaure: tensor([47.7016], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 2.547144511931162
max eps sqaure: tensor([47.7016], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 2.547144511931162
max eps sqaure: tensor([47.7016], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 2.547144511931162
max eps sqaure: tensor([47.7016], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 2.547144511931162
max eps sqaure: tensor([47.7016], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 2.547144511931162
max eps sqaure: tensor([47.7016], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 2.547144511931162
max eps sqaure: tensor([47.7016], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 2.547144511931162
max eps sqaure: tensor([47.7016], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 2.547144511931162
max eps sqaure: tensor([47.7016], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 2.547144511931162
max eps sqaure: tensor([47.7016], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 2.547144511931162
max eps sqaure: tensor([47.7016], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 2.547144511931162
max eps sqaure: tensor([47.7016], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 2.547144511931162
max eps sqaure: tensor([47.7016], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 2.547144511931162
max eps sqaure: tensor([47.7016], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 2.547144511931162
max eps sqaure: tensor([47.7016], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 2.547144511931162
max eps sqaure: tensor([47.7016], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 2.547144511931162
max eps sqaure: tensor([47.7016], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 2.547144511931162
max eps sqaure: tensor([47.7016], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 2.547144511931162
max eps sqaure: tensor([47.7016], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
train_loss: 2.547144511931162
max eps sqaure: tensor([47.7016], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
---> Checking nodes...1/ 1600.00 
TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)
    --> Start regularising_EU_LG_UA
train_loss: 18678.05200432082
regular term: 2821.618678101536
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Restore model and lr decrease
train_loss: 644.9447553882973
regular term: 2821.618696241377
Try trim model: Copy model and delete nodes success
    --> Start module_EU_LG
train_loss: 2155.2464056409767
max eps sqaure: tensor([3064.3200], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)
Save model and lr increase
#######################################
2024-01-06 17:05:22.643453#######################################
### New lts ###
Total obtaining n: 328
obtaining n over lr goal: 0
Total select n: 333
select n over lr goal: 5
//////////// Start module_EU_LG_UA Epoch ///////////Finish epoch. non acceptable module at max eps tensor([9.8415], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)////////// Start CRAM and REORG with unacceptable wt //////////model after wt: TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=1, bias=True)
  (layer_out): Linear(in_features=1, out_features=1, bias=True)
  (relu): ReLU()
)eps_square (last 10) before cram: tensor([6.9044, 7.2054, 7.2848, 7.3423, 7.3833, 7.5202, 7.6095, 7.6147, 7.6644,
        7.6826], device='cuda:0', dtype=torch.float64, grad_fn=<ViewBackward0>)cramming sample 328th | total of 5cramming sample 329th | total of 5cramming sample 330th | total of 5cramming sample 331th | total of 5cramming sample 332th | total of 5model after cram: TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)eps_sqaure (last 10) after cram: tensor([6.9044e+00, 7.2054e+00, 7.2848e+00, 7.3423e+00, 7.3833e+00, 4.0713e-23,
        7.1255e-23, 2.3381e-22, 1.1059e-24, 1.6060e-23], device='cuda:0',
       dtype=torch.float64, grad_fn=<ViewBackward0>)Acceptable SLFN exist in 'acceptable/cram.pth'=================== reorganising ===================---> Checking nodes...0/ 1600.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 18307.960000068633regular term: 2821.617494157309Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 2517.3843196478633max eps sqaure: tensor([3989.6071], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2417.158078636446max eps sqaure: tensor([3611.6931], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2301.7981926834896max eps sqaure: tensor([3340.1028], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2169.8644291877376max eps sqaure: tensor([3116.0669], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2019.964016519948max eps sqaure: tensor([2857.9340], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1850.5574837902338max eps sqaure: tensor([2561.4966], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1660.7424406874745max eps sqaure: tensor([2222.9832], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1451.7359965347175max eps sqaure: tensor([1951.6772], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1230.5440288231139max eps sqaure: tensor([1853.5566], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1016.5185882974579max eps sqaure: tensor([1834.8093], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 839.6761813247532max eps sqaure: tensor([1800.0191], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 710.8731417262012max eps sqaure: tensor([1673.4061], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 591.0755692523575max eps sqaure: tensor([1401.0616], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 451.31146411076276max eps sqaure: tensor([1073.9630], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 294.1467744662038max eps sqaure: tensor([621.7148], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 139.991621803938max eps sqaure: tensor([415.4721], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 40.31486175140072max eps sqaure: tensor([294.5585], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 21.994152978492355max eps sqaure: tensor([223.4218], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 18.28803523363212max eps sqaure: tensor([162.4848], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 12.387762202175347max eps sqaure: tensor([129.8165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 9.359993949051086max eps sqaure: tensor([82.4483], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 7.70617521594811max eps sqaure: tensor([54.1770], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.889547357831755max eps sqaure: tensor([41.3387], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.234876541934017max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.405790279707845max eps sqaure: tensor([62.5396], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.883800200807761max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.883800200807761max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.883800200807761max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.883800200807761max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.883800200807761max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.883800200807761max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.883800200807761max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.883800200807761max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.883800200807761max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.883800200807761max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.883800200807761max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.883800200807761max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.883800200807761max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.883800200807761max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.883800200807761max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.883800200807761max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.883800200807761max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.883800200807761max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.883800200807761max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.883800200807761max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.883800200807761max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.883800200807761max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.883800200807761max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.883800200807761max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.883800200807761max eps sqaure: tensor([35.0165], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increase---> Checking nodes...1/ 1600.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 18307.960000068633regular term: 2821.617494157309Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Restore model and lr decreasetrain_loss: 644.944484923824regular term: 2821.617510358658Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 2498.7655458271524max eps sqaure: tensor([3689.6248], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2391.3238123123033max eps sqaure: tensor([3501.6333], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2267.215275746896max eps sqaure: tensor([3283.1013], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2124.7857972091665max eps sqaure: tensor([3030.4688], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1962.3264024089856max eps sqaure: tensor([2739.8851], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1778.543958069253max eps sqaure: tensor([2406.3881], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1574.559991839278max eps sqaure: tensor([2111.5094], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1355.605735461433max eps sqaure: tensor([1930.9181], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increase


#######################################2024-01-06 17:05:58.630265########################################## New lts ###Total obtaining n: 328
obtaining n over lr goal: 0
Total select n: 333
select n over lr goal: 5
//////////// Start module_EU_LG_UA Epoch ///////////Finish epoch. non acceptable module at max eps tensor([9.8415], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)////////// Start CRAM and REORG with unacceptable wt //////////model after wt: TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=1, bias=True)
  (layer_out): Linear(in_features=1, out_features=1, bias=True)
  (relu): ReLU()
)eps_square (last 10) before cram: tensor([6.9044, 7.2054, 7.2848, 7.3423, 7.3833, 7.5202, 7.6095, 7.6147, 7.6644,
        7.6826], device='cuda:0', dtype=torch.float64, grad_fn=<ViewBackward0>)cramming sample 328th | total of 5cramming sample 329th | total of 5cramming sample 330th | total of 5cramming sample 331th | total of 5cramming sample 332th | total of 5model after cram: TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)eps_sqaure (last 10) after cram: tensor([6.9044e+00, 7.2054e+00, 7.2848e+00, 7.3423e+00, 7.3833e+00, 2.2265e-24,
        6.1142e-24, 5.4607e-25, 3.5624e-25, 1.6003e-23], device='cuda:0',
       dtype=torch.float64, grad_fn=<ViewBackward0>)Acceptable SLFN exist in 'acceptable/cram.pth'=================== reorganising ===================---> Checking nodes...0/ 1600.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 22618.27412103755regular term: 2821.616900451482Restore model and lr decreasetrain_loss: 644.9443531212407regular term: 2821.6169324550083Restore model and lr decreasetrain_loss: 644.9443531212407regular term: 2821.6169324550083Restore model and lr decreasetrain_loss: 644.9443531212407regular term: 2821.6169324550083Restore model and lr decreasetrain_loss: 644.9443531212407regular term: 2821.6169324550083Restore model and lr decreasetrain_loss: 644.9443531212407regular term: 2821.6169324550083Restore model and lr decreasetrain_loss: 644.9443531212407regular term: 2821.6169324550083Restore model and lr decreasetrain_loss: 644.9443531212407regular term: 2821.6169324550083Restore model and lr decreasetrain_loss: 644.9443531212407regular term: 2821.6169324550083Restore model and lr decreasetrain_loss: 644.9443531212407regular term: 2821.6169324550083Restore model and lr decreasetrain_loss: 644.9443531212407regular term: 2821.6169324550083Restore model and lr decreasetrain_loss: 644.9443531212407regular term: 2821.6169324550083Restore model and lr decreasetrain_loss: 644.9443531212407regular term: 2821.6169324550083Restore model and lr decreasetrain_loss: 644.9443531212407regular term: 2821.6169324550083Restore model and lr decreasetrain_loss: 644.9443531212407regular term: 2821.6169324550083Restore model and lr decreasetrain_loss: 644.9443531212407regular term: 2821.6169324550083Restore model and lr decreasetrain_loss: 644.9443531212407regular term: 2821.6169324550083Restore model and lr decreasetrain_loss: 644.9443531212407regular term: 2821.6169324550083Restore model and lr decreasetrain_loss: 644.9443531212407regular term: 2821.6169324550083Restore model and lr decreasetrain_loss: 644.9443531212407regular term: 2821.6169324550083Restore model and lr decreasetrain_loss: 644.9443531212407regular term: 2821.6169324550083Restore model and lr decreasetrain_loss: 644.9443531212407regular term: 2821.6169324550083Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 2541.382161991837max eps sqaure: tensor([4169.4796], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2436.9385031735883max eps sqaure: tensor([3786.9003], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2316.5185820029033max eps sqaure: tensor([3357.8061], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2178.5898552735475max eps sqaure: tensor([3092.6134], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2021.6253611692844max eps sqaure: tensor([2832.5001], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1844.2085190746138max eps sqaure: tensor([2530.5826], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1646.1870428649565max eps sqaure: tensor([2183.4563], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1430.3439585388157max eps sqaure: tensor([2010.4879], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1205.399853280037max eps sqaure: tensor([2008.9695], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 990.1388748658719max eps sqaure: tensor([1968.6631], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 810.6295384073538max eps sqaure: tensor([1918.2853], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 678.2540864761364max eps sqaure: tensor([1814.5336], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 565.2258111295017max eps sqaure: tensor([1612.7086], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 437.5832406095919max eps sqaure: tensor([1296.1386], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increase#######################################2024-01-06 17:06:18.427003#######################################


### New lts ###Total obtaining n: 328
obtaining n over lr goal: 0
Total select n: 333
select n over lr goal: 5
//////////// Start module_EU_LG_UA Epoch ///////////Finish epoch. non acceptable module at max eps tensor([9.8415], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)////////// Start CRAM and REORG with unacceptable wt //////////model after wt: TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=1, bias=True)
  (layer_out): Linear(in_features=1, out_features=1, bias=True)
  (relu): ReLU()
)eps_square (last 10) before cram: tensor([6.9044, 7.2054, 7.2848, 7.3423, 7.3833, 7.5202, 7.6095, 7.6147, 7.6644,
        7.6826], device='cuda:0', dtype=torch.float64, grad_fn=<ViewBackward0>)cramming sample 328th | total of 5cramming sample 329th | total of 5cramming sample 330th | total of 5cramming sample 331th | total of 5cramming sample 332th | total of 5model after cram: TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)eps_sqaure (last 10) after cram: tensor([6.9044e+00, 7.2054e+00, 7.2848e+00, 7.3423e+00, 7.3833e+00, 4.0804e-23,
        7.1255e-23, 5.4607e-25, 1.1059e-24, 4.1900e-23], device='cuda:0',
       dtype=torch.float64, grad_fn=<ViewBackward0>)Acceptable SLFN exist in 'acceptable/cram.pth'=================== reorganising ===================---> Checking nodes...0/ 1600.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 28446.818877571688regular term: 2821.6211196971367Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 2474.378046450785max eps sqaure: tensor([3780.6956], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2368.7013868680274max eps sqaure: tensor([3437.2576], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2246.6068695857425max eps sqaure: tensor([3201.7724], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2106.008414922197max eps sqaure: tensor([2960.6878], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1944.9460102615449max eps sqaure: tensor([2677.0620], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1762.244895139548max eps sqaure: tensor([2345.4445], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1558.462060667328max eps sqaure: tensor([2049.8995], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1338.431506044528max eps sqaure: tensor([1921.1794], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1114.7092605039727max eps sqaure: tensor([1899.2582], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 910.6895066786514max eps sqaure: tensor([1882.1305], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 751.0591810626329max eps sqaure: tensor([1825.9836], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 632.2899418938089max eps sqaure: tensor([1673.6915], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 513.8690385705219max eps sqaure: tensor([1370.3777], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 372.69623977080937max eps sqaure: tensor([953.1865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 214.44126632353263max eps sqaure: tensor([487.1358], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 77.55911383602952max eps sqaure: tensor([294.1731], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 23.755074850220478max eps sqaure: tensor([149.2092], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 20.465299114307417max eps sqaure: tensor([151.5253], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 15.64437428244227max eps sqaure: tensor([103.5845], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 11.918199179030738max eps sqaure: tensor([86.6139], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 10.051450861360326max eps sqaure: tensor([64.8544], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 8.597827949653292max eps sqaure: tensor([57.3584], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 7.436565070425888max eps sqaure: tensor([47.8706], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.243928445608642max eps sqaure: tensor([35.3388], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.321775463070353max eps sqaure: tensor([30.7985], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.788036726562176max eps sqaure: tensor([54.1012], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.388142970035306max eps sqaure: tensor([68.8975], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.6301276782795995max eps sqaure: tensor([35.7739], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.9445541598742477max eps sqaure: tensor([45.6607], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.0075768744595828max eps sqaure: tensor([50.2008], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 2.5646609440928505max eps sqaure: tensor([45.6607], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.5646609440928505max eps sqaure: tensor([45.6607], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.5646609440928505max eps sqaure: tensor([45.6607], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.5646609440928505max eps sqaure: tensor([45.6607], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.5646609440928505max eps sqaure: tensor([45.6607], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.5646609440928505max eps sqaure: tensor([45.6607], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.5646609440928505max eps sqaure: tensor([45.6607], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.5646609440928505max eps sqaure: tensor([45.6607], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.5646609440928505max eps sqaure: tensor([45.6607], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.5646609440928505max eps sqaure: tensor([45.6607], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.5646609440928505max eps sqaure: tensor([45.6607], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.5646609440928505max eps sqaure: tensor([45.6607], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.5646609440928505max eps sqaure: tensor([45.6607], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.5646609440928505max eps sqaure: tensor([45.6607], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.5646609440928505max eps sqaure: tensor([45.6607], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.5646609440928505max eps sqaure: tensor([45.6607], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.5646609440928505max eps sqaure: tensor([45.6607], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.5646609440928505max eps sqaure: tensor([45.6607], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.5646609440928505max eps sqaure: tensor([45.6607], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.5646609440928505max eps sqaure: tensor([45.6607], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increase---> Checking nodes...1/ 1600.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 28446.818877571688regular term: 2821.6211196971367Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 3025.2504801391146max eps sqaure: tensor([5408.1609], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2896.951130898496max eps sqaure: tensor([4951.1990], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2750.441318301184max eps sqaure: tensor([4444.3141], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2584.8130719830565max eps sqaure: tensor([3889.9350], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2399.363989379589max eps sqaure: tensor([3396.6306], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2192.8900626975656max eps sqaure: tensor([2926.5877], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1964.7344492454142max eps sqaure: tensor([2509.4930], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1715.7005781514488max eps sqaure: tensor([2080.7369], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1450.0856022103792max eps sqaure: tensor([1810.9140], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1180.3096956053182max eps sqaure: tensor([1641.5861], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 930.0153198197667max eps sqaure: tensor([1513.7719], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 724.8659596066491max eps sqaure: tensor([1337.5604], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 554.1836021206879max eps sqaure: tensor([1061.1342], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 376.00743797024955max eps sqaure: tensor([691.0251], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 199.93678866907055max eps sqaure: tensor([342.8479], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 75.41806843576755max eps sqaure: tensor([236.5849], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 22.70051662865389max eps sqaure: tensor([132.0574], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 15.22713180030915max eps sqaure: tensor([103.0356], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 13.088014670873008max eps sqaure: tensor([84.6752], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 9.663557669408565max eps sqaure: tensor([69.1018], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 8.350275125229729max eps sqaure: tensor([56.0368], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 7.145414597164002max eps sqaure: tensor([44.8230], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.5797001382169755max eps sqaure: tensor([40.8102], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.949681112184332max eps sqaure: tensor([34.3885], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.497699949503449max eps sqaure: tensor([29.6817], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.575530746568254max eps sqaure: tensor([27.4186], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.829715081253607max eps sqaure: tensor([29.6817], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.829715081253607max eps sqaure: tensor([29.6817], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.829715081253607max eps sqaure: tensor([29.6817], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.829715081253607max eps sqaure: tensor([29.6817], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.829715081253607max eps sqaure: tensor([29.6817], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.829715081253607max eps sqaure: tensor([29.6817], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.829715081253607max eps sqaure: tensor([29.6817], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.829715081253607max eps sqaure: tensor([29.6817], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.829715081253607max eps sqaure: tensor([29.6817], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.829715081253607max eps sqaure: tensor([29.6817], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.829715081253607max eps sqaure: tensor([29.6817], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.829715081253607max eps sqaure: tensor([29.6817], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.829715081253607max eps sqaure: tensor([29.6817], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.829715081253607max eps sqaure: tensor([29.6817], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.829715081253607max eps sqaure: tensor([29.6817], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.829715081253607max eps sqaure: tensor([29.6817], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.829715081253607max eps sqaure: tensor([29.6817], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.829715081253607max eps sqaure: tensor([29.6817], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.829715081253607max eps sqaure: tensor([29.6817], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.829715081253607max eps sqaure: tensor([29.6817], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.829715081253607max eps sqaure: tensor([29.6817], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.829715081253607max eps sqaure: tensor([29.6817], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.829715081253607max eps sqaure: tensor([29.6817], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.829715081253607max eps sqaure: tensor([29.6817], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increase---> Checking nodes...2/ 1600.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 28446.818877571688regular term: 2821.6211196971367Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 1933.1437605120348max eps sqaure: tensor([2749.7910], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1836.6749233140752max eps sqaure: tensor([2575.9708], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1726.003551757681max eps sqaure: tensor([2374.2049], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1600.3834252113872max eps sqaure: tensor([2142.2420], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1459.651617245667max eps sqaure: tensor([1890.3211], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1305.0238334158869max eps sqaure: tensor([1745.0144], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1140.0091077623738max eps sqaure: tensor([1686.7568], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 971.5344537927646max eps sqaure: tensor([1630.6773], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 811.1715917207463max eps sqaure: tensor([1564.5444], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 670.0655855674744max eps sqaure: tensor([1453.0779], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 547.4252533308033max eps sqaure: tensor([1267.6147], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 425.510107706274max eps sqaure: tensor([989.4506], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 290.4986298586752max eps sqaure: tensor([667.1937], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 150.29832570888232max eps sqaure: tensor([275.0451], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 51.85610189886999max eps sqaure: tensor([214.0884], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 15.758684723566498max eps sqaure: tensor([98.2128], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 12.786027036217988max eps sqaure: tensor([107.5398], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 10.06819140309536max eps sqaure: tensor([67.7847], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 7.4285919883109015max eps sqaure: tensor([55.0442], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.611353338209138max eps sqaure: tensor([43.3987], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.75045339522683max eps sqaure: tensor([35.8925], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.352999359413947max eps sqaure: tensor([34.3478], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.921909523097742max eps sqaure: tensor([29.4730], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.68589872660676max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.774444300127232max eps sqaure: tensor([23.3488], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.3563775067620303max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3563775067620303max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3563775067620303max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3563775067620303max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3563775067620303max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3563775067620303max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3563775067620303max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3563775067620303max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3563775067620303max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3563775067620303max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3563775067620303max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3563775067620303max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3563775067620303max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3563775067620303max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3563775067620303max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3563775067620303max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3563775067620303max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3563775067620303max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3563775067620303max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3563775067620303max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3563775067620303max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3563775067620303max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3563775067620303max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3563775067620303max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3563775067620303max eps sqaure: tensor([26.4638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increase---> Checking nodes...3/ 1600.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 28446.818877571688regular term: 2821.6211196971367Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 2275.902284871316max eps sqaure: tensor([3355.8606], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2170.7715644660684max eps sqaure: tensor([3057.1497], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2050.143319000654max eps sqaure: tensor([2851.6975], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1912.8711587995604max eps sqaure: tensor([2614.9414], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1757.8184044722177max eps sqaure: tensor([2342.0101], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1584.224088815886max eps sqaure: tensor([2027.4549], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1392.5402336302932max eps sqaure: tensor([1784.8293], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1187.1337331620173max eps sqaure: tensor([1664.6232], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 979.9916278015793max eps sqaure: tensor([1611.7943], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 791.0658126760128max eps sqaure: tensor([1545.3980], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 634.2833895792136max eps sqaure: tensor([1403.9560], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 493.8003089179638max eps sqaure: tensor([1151.9936], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 345.3262266792961max eps sqaure: tensor([809.1227], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 198.6573504776018max eps sqaure: tensor([450.4497], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 85.24336998848345max eps sqaure: tensor([207.1952], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 26.576377736314925max eps sqaure: tensor([127.0058], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 13.973480820124392max eps sqaure: tensor([81.3219], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 12.559429378680173max eps sqaure: tensor([59.8898], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 8.720282843540753max eps sqaure: tensor([53.8211], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 7.082481724744149max eps sqaure: tensor([47.5362], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.807220202263681max eps sqaure: tensor([35.7466], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.438736403981807max eps sqaure: tensor([30.8969], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.072140124154503max eps sqaure: tensor([25.7242], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.778499819560239max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.798294711216333max eps sqaure: tensor([23.5756], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.252692133072777max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.252692133072777max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.252692133072777max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.252692133072777max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.252692133072777max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.252692133072777max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.252692133072777max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.252692133072777max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.252692133072777max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.252692133072777max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.252692133072777max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.252692133072777max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.252692133072777max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.252692133072777max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.252692133072777max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.252692133072777max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.252692133072777max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.252692133072777max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.252692133072777max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.252692133072777max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.252692133072777max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.252692133072777max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.252692133072777max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.252692133072777max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.252692133072777max eps sqaure: tensor([22.6687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increase---> Checking nodes...4/ 1600.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 28446.818877571688regular term: 2821.6211196971367Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 2154.3059023965684max eps sqaure: tensor([3076.9724], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2037.1339182903787max eps sqaure: tensor([2865.2873], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1903.3140521453279max eps sqaure: tensor([2622.3121], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1752.3151212232904max eps sqaure: tensor([2347.0790], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1584.8446319365255max eps sqaure: tensor([2040.6305], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1403.3106844992697max eps sqaure: tensor([1840.1797], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1212.6940482149541max eps sqaure: tensor([1729.4625], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1021.5512838476283max eps sqaure: tensor([1661.9617], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 842.4885088725454max eps sqaure: tensor([1573.1047], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 687.8266913860547max eps sqaure: tensor([1450.7208], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 561.1304202115792max eps sqaure: tensor([1277.4560], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 445.7490215310964max eps sqaure: tensor([1072.2581], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 322.0483975728495max eps sqaure: tensor([815.3394], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 183.89915955193592max eps sqaure: tensor([386.8018], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 72.60899059096535max eps sqaure: tensor([216.8493], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 21.234244157733727max eps sqaure: tensor([130.5521], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 11.836542784234657max eps sqaure: tensor([82.7326], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 11.06718710960542max eps sqaure: tensor([63.1015], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 7.9653807204043225max eps sqaure: tensor([59.0364], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.703274717026726max eps sqaure: tensor([44.3858], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.728753290888439max eps sqaure: tensor([34.9798], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.9547038954015745max eps sqaure: tensor([30.2719], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.346652481874917max eps sqaure: tensor([27.2701], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.8621503942067275max eps sqaure: tensor([23.7851], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.5190648206738504max eps sqaure: tensor([19.0897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.6385500912904303max eps sqaure: tensor([31.6357], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 2.4068151517304384max eps sqaure: tensor([19.0897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.4068151517304384max eps sqaure: tensor([19.0897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.4068151517304384max eps sqaure: tensor([19.0897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.4068151517304384max eps sqaure: tensor([19.0897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.4068151517304384max eps sqaure: tensor([19.0897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.4068151517304384max eps sqaure: tensor([19.0897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.4068151517304384max eps sqaure: tensor([19.0897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.4068151517304384max eps sqaure: tensor([19.0897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.4068151517304384max eps sqaure: tensor([19.0897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.4068151517304384max eps sqaure: tensor([19.0897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.4068151517304384max eps sqaure: tensor([19.0897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.4068151517304384max eps sqaure: tensor([19.0897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.4068151517304384max eps sqaure: tensor([19.0897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.4068151517304384max eps sqaure: tensor([19.0897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.4068151517304384max eps sqaure: tensor([19.0897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.4068151517304384max eps sqaure: tensor([19.0897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.4068151517304384max eps sqaure: tensor([19.0897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.4068151517304384max eps sqaure: tensor([19.0897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.4068151517304384max eps sqaure: tensor([19.0897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.4068151517304384max eps sqaure: tensor([19.0897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.4068151517304384max eps sqaure: tensor([19.0897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.4068151517304384max eps sqaure: tensor([19.0897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.4068151517304384max eps sqaure: tensor([19.0897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.4068151517304384max eps sqaure: tensor([19.0897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increase---> Checking nodes...5/ 1600.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 28446.818877571688regular term: 2821.6211196971367Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 2311.592650782183max eps sqaure: tensor([3389.2879], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2190.578810229758max eps sqaure: tensor([3163.9796], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2051.4407708002113max eps sqaure: tensor([2902.9254], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1893.1612966842088max eps sqaure: tensor([2603.8189], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1715.9992880363886max eps sqaure: tensor([2314.3991], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1522.3946776251075max eps sqaure: tensor([2134.8874], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1317.901737351493max eps sqaure: tensor([1995.6754], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1112.5461918335886max eps sqaure: tensor([1952.1872], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 923.0725441694393max eps sqaure: tensor([1902.0058], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 767.2478835141407max eps sqaure: tensor([1815.8556], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 650.426022924812max eps sqaure: tensor([1684.2513], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 552.6724067650574max eps sqaure: tensor([1469.1944], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 442.9026955560574max eps sqaure: tensor([1164.6333], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 301.108839129437max eps sqaure: tensor([790.0175], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 146.30813568105626max eps sqaure: tensor([313.8401], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 47.944780372333945max eps sqaure: tensor([222.9839], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 16.55113644932219max eps sqaure: tensor([114.8949], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 14.59161444095625max eps sqaure: tensor([72.8848], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 10.387587933054684max eps sqaure: tensor([72.6903], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 7.87724657279539max eps sqaure: tensor([55.8661], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.679842928774694max eps sqaure: tensor([45.8269], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.505677233435062max eps sqaure: tensor([36.7996], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.825006225177277max eps sqaure: tensor([33.8312], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.297662079291114max eps sqaure: tensor([28.1453], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.9697089036783932max eps sqaure: tensor([21.0084], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.329779729068487max eps sqaure: tensor([34.1907], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 2.792314455691939max eps sqaure: tensor([21.0084], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.792314455691939max eps sqaure: tensor([21.0084], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.792314455691939max eps sqaure: tensor([21.0084], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.792314455691939max eps sqaure: tensor([21.0084], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.792314455691939max eps sqaure: tensor([21.0084], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.792314455691939max eps sqaure: tensor([21.0084], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.792314455691939max eps sqaure: tensor([21.0084], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.792314455691939max eps sqaure: tensor([21.0084], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.792314455691939max eps sqaure: tensor([21.0084], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.792314455691939max eps sqaure: tensor([21.0084], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.792314455691939max eps sqaure: tensor([21.0084], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.792314455691939max eps sqaure: tensor([21.0084], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.792314455691939max eps sqaure: tensor([21.0084], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.792314455691939max eps sqaure: tensor([21.0084], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.792314455691939max eps sqaure: tensor([21.0084], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.792314455691939max eps sqaure: tensor([21.0084], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.792314455691939max eps sqaure: tensor([21.0084], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.792314455691939max eps sqaure: tensor([21.0084], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.792314455691939max eps sqaure: tensor([21.0084], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.792314455691939max eps sqaure: tensor([21.0084], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.792314455691939max eps sqaure: tensor([21.0084], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.792314455691939max eps sqaure: tensor([21.0084], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.792314455691939max eps sqaure: tensor([21.0084], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.792314455691939max eps sqaure: tensor([21.0084], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increase---> Checking nodes...6/ 1600.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 28446.818877571688regular term: 2821.6211196971367Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 2791.711879656666max eps sqaure: tensor([4240.6208], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2655.602456660523max eps sqaure: tensor([3853.7124], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2499.559737654509max eps sqaure: tensor([3543.1953], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2322.4197420320193max eps sqaure: tensor([3229.5514], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2123.7249946775664max eps sqaure: tensor([2877.8678], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1903.9500159811848max eps sqaure: tensor([2488.1788], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1665.3147421921285max eps sqaure: tensor([2170.4471], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1413.618282057326max eps sqaure: tensor([1940.5696], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1161.0031556982192max eps sqaure: tensor([1810.4247], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 927.5109839985636max eps sqaure: tensor([1687.5825], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 735.4027039684365max eps sqaure: tensor([1528.8100], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 581.9248559346921max eps sqaure: tensor([1304.5467], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 435.396270467133max eps sqaure: tensor([1004.4995], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 284.20649929840164max eps sqaure: tensor([653.6055], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 146.560419321927max eps sqaure: tensor([325.0850], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 53.35221250146478max eps sqaure: tensor([189.0845], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 17.82368264744684max eps sqaure: tensor([119.8229], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 14.274306930964208max eps sqaure: tensor([101.6586], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 11.334957794185847max eps sqaure: tensor([59.0111], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 8.506130089867623max eps sqaure: tensor([63.1704], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 7.098556637456135max eps sqaure: tensor([53.6128], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.185011267908865max eps sqaure: tensor([42.0667], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.535343531669156max eps sqaure: tensor([35.8250], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.849551482222168max eps sqaure: tensor([27.4232], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.3600332385059755max eps sqaure: tensor([23.1809], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.0820500709623495max eps sqaure: tensor([25.4265], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.254807176401983max eps sqaure: tensor([43.0734], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 2.586426815952352max eps sqaure: tensor([25.4265], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.586426815952352max eps sqaure: tensor([25.4265], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.586426815952352max eps sqaure: tensor([25.4265], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.586426815952352max eps sqaure: tensor([25.4265], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.586426815952352max eps sqaure: tensor([25.4265], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.586426815952352max eps sqaure: tensor([25.4265], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.586426815952352max eps sqaure: tensor([25.4265], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.586426815952352max eps sqaure: tensor([25.4265], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.586426815952352max eps sqaure: tensor([25.4265], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.586426815952352max eps sqaure: tensor([25.4265], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.586426815952352max eps sqaure: tensor([25.4265], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.586426815952352max eps sqaure: tensor([25.4265], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.586426815952352max eps sqaure: tensor([25.4265], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.586426815952352max eps sqaure: tensor([25.4265], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.586426815952352max eps sqaure: tensor([25.4265], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.586426815952352max eps sqaure: tensor([25.4265], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.586426815952352max eps sqaure: tensor([25.4265], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.586426815952352max eps sqaure: tensor([25.4265], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.586426815952352max eps sqaure: tensor([25.4265], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.586426815952352max eps sqaure: tensor([25.4265], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.586426815952352max eps sqaure: tensor([25.4265], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.586426815952352max eps sqaure: tensor([25.4265], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.586426815952352max eps sqaure: tensor([25.4265], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increase---> Checking nodes...7/ 1600.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 28446.818877571688regular term: 2821.6211196971367Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 2433.239024240376max eps sqaure: tensor([3536.7958], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2308.953510725088max eps sqaure: tensor([3265.8345], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2166.849222252487max eps sqaure: tensor([3019.2101], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2005.9650338625404max eps sqaure: tensor([2737.8331], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1826.0940575837694max eps sqaure: tensor([2419.9535], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1628.3352651784555max eps sqaure: tensor([2086.0553], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1415.839293071271max eps sqaure: tensor([1887.9269], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1195.1329309337116max eps sqaure: tensor([1760.8000], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 978.92776129064max eps sqaure: tensor([1676.7881], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 783.7835200146787max eps sqaure: tensor([1562.1402], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 619.7019014503672max eps sqaure: tensor([1390.5535], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 477.4942873742055max eps sqaure: tensor([1134.1563], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 336.13251811475624max eps sqaure: tensor([795.7620], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 194.60709804477403max eps sqaure: tensor([439.2661], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 82.2605449069438max eps sqaure: tensor([219.0078], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 25.114277370953733max eps sqaure: tensor([141.8861], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 13.03906606592099max eps sqaure: tensor([83.0944], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 12.063519988310238max eps sqaure: tensor([60.8393], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 8.660600605313197max eps sqaure: tensor([52.7320], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 7.221790043964023max eps sqaure: tensor([50.7234], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.223454726399869max eps sqaure: tensor([39.2426], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.562038481553697max eps sqaure: tensor([33.2554], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.958992430618544max eps sqaure: tensor([29.1849], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.543401870747498max eps sqaure: tensor([22.8514], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.384635336756446max eps sqaure: tensor([21.5550], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.404957118819019max eps sqaure: tensor([27.7019], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 2.724243711263514max eps sqaure: tensor([21.5550], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.724243711263514max eps sqaure: tensor([21.5550], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.724243711263514max eps sqaure: tensor([21.5550], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.724243711263514max eps sqaure: tensor([21.5550], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.724243711263514max eps sqaure: tensor([21.5550], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.724243711263514max eps sqaure: tensor([21.5550], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.724243711263514max eps sqaure: tensor([21.5550], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.724243711263514max eps sqaure: tensor([21.5550], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.724243711263514max eps sqaure: tensor([21.5550], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.724243711263514max eps sqaure: tensor([21.5550], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.724243711263514max eps sqaure: tensor([21.5550], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.724243711263514max eps sqaure: tensor([21.5550], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.724243711263514max eps sqaure: tensor([21.5550], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.724243711263514max eps sqaure: tensor([21.5550], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.724243711263514max eps sqaure: tensor([21.5550], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.724243711263514max eps sqaure: tensor([21.5550], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.724243711263514max eps sqaure: tensor([21.5550], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.724243711263514max eps sqaure: tensor([21.5550], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.724243711263514max eps sqaure: tensor([21.5550], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.724243711263514max eps sqaure: tensor([21.5550], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.724243711263514max eps sqaure: tensor([21.5550], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.724243711263514max eps sqaure: tensor([21.5550], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.724243711263514max eps sqaure: tensor([21.5550], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.724243711263514max eps sqaure: tensor([21.5550], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increase---> Checking nodes...8/ 1600.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 28446.818877571688regular term: 2821.6211196971367Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 2021.9668061294808max eps sqaure: tensor([2794.5379], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1909.872704123012max eps sqaure: tensor([2597.8762], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1782.1555768442315max eps sqaure: tensor([2372.2647], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1638.4283566018896max eps sqaure: tensor([2116.7663], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1479.3746864733146max eps sqaure: tensor([1872.8315], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1307.2831793463865max eps sqaure: tensor([1709.9014], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1126.937630416857max eps sqaure: tensor([1598.0394], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 946.1823291193947max eps sqaure: tensor([1531.5064], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 776.8162515660525max eps sqaure: tensor([1451.0888], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 630.731148406399max eps sqaure: tensor([1340.8510], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 510.74356091526073max eps sqaure: tensor([1193.2827], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 399.8042147195409max eps sqaure: tensor([982.7341], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 275.2103395742927max eps sqaure: tensor([678.6525], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 143.562369879336max eps sqaure: tensor([283.5967], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 49.25356117508233max eps sqaure: tensor([185.0939], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 14.31794269745417max eps sqaure: tensor([92.6621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 11.20792831686317max eps sqaure: tensor([68.8579], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 9.251255268721946max eps sqaure: tensor([62.6966], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.925054522799351max eps sqaure: tensor([53.7595], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.120192836650077max eps sqaure: tensor([42.4924], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.313645273872097max eps sqaure: tensor([35.2909], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.761556384639854max eps sqaure: tensor([32.7759], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.179346429819561max eps sqaure: tensor([29.2768], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.763986906035388max eps sqaure: tensor([26.3512], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.5273812607751758max eps sqaure: tensor([21.4865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.602262835758678max eps sqaure: tensor([24.8390], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 2.437040518630234max eps sqaure: tensor([21.4865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.437040518630234max eps sqaure: tensor([21.4865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.437040518630234max eps sqaure: tensor([21.4865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.437040518630234max eps sqaure: tensor([21.4865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.437040518630234max eps sqaure: tensor([21.4865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.437040518630234max eps sqaure: tensor([21.4865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.437040518630234max eps sqaure: tensor([21.4865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.437040518630234max eps sqaure: tensor([21.4865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.437040518630234max eps sqaure: tensor([21.4865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.437040518630234max eps sqaure: tensor([21.4865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.437040518630234max eps sqaure: tensor([21.4865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.437040518630234max eps sqaure: tensor([21.4865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.437040518630234max eps sqaure: tensor([21.4865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.437040518630234max eps sqaure: tensor([21.4865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.437040518630234max eps sqaure: tensor([21.4865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.437040518630234max eps sqaure: tensor([21.4865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.437040518630234max eps sqaure: tensor([21.4865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.437040518630234max eps sqaure: tensor([21.4865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.437040518630234max eps sqaure: tensor([21.4865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.437040518630234max eps sqaure: tensor([21.4865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.437040518630234max eps sqaure: tensor([21.4865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.437040518630234max eps sqaure: tensor([21.4865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.437040518630234max eps sqaure: tensor([21.4865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.437040518630234max eps sqaure: tensor([21.4865], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increase---> Checking nodes...9/ 1600.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 28446.818877571688regular term: 2821.6211196971367Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 2403.7545215682467max eps sqaure: tensor([3505.9481], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2277.056706429061max eps sqaure: tensor([3210.8305], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2132.9759763747784max eps sqaure: tensor([2957.1008], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1970.9142961353784max eps sqaure: tensor([2669.2945], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1791.0849090429308max eps sqaure: tensor([2347.5494], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1595.406244400012max eps sqaure: tensor([2129.9136], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1388.1573477853944max eps sqaure: tensor([1986.0057], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1176.2239637875234max eps sqaure: tensor([1932.8778], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 972.9173052868506max eps sqaure: tensor([1873.1584], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 794.0118809402139max eps sqaure: tensor([1790.2621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 651.9165112728766max eps sqaure: tensor([1668.0434], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 539.7937718346999max eps sqaure: tensor([1464.7373], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 430.81266197406876max eps sqaure: tensor([1153.7583], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 300.3296657828543max eps sqaure: tensor([752.5523], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 161.0552239670702max eps sqaure: tensor([348.4768], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 57.08285010394196max eps sqaure: tensor([252.6748], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 19.541699952485875max eps sqaure: tensor([131.9942], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 13.848666692593932max eps sqaure: tensor([78.6821], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 11.195028907359822max eps sqaure: tensor([72.4103], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 8.096289614000158max eps sqaure: tensor([62.8215], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.8031538909914575max eps sqaure: tensor([57.2887], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.795265920126444max eps sqaure: tensor([44.7822], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.982396698757628max eps sqaure: tensor([38.2973], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.2384286952476815max eps sqaure: tensor([29.2262], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.6979966844323298max eps sqaure: tensor([21.1074], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3040631318283236max eps sqaure: tensor([18.7818], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.167648220459545max eps sqaure: tensor([39.0872], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.1923479576507963max eps sqaure: tensor([64.1361], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 2.2516107276865593max eps sqaure: tensor([39.0872], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2516107276865593max eps sqaure: tensor([39.0872], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2516107276865593max eps sqaure: tensor([39.0872], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2516107276865593max eps sqaure: tensor([39.0872], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2516107276865593max eps sqaure: tensor([39.0872], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2516107276865593max eps sqaure: tensor([39.0872], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2516107276865593max eps sqaure: tensor([39.0872], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2516107276865593max eps sqaure: tensor([39.0872], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2516107276865593max eps sqaure: tensor([39.0872], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2516107276865593max eps sqaure: tensor([39.0872], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2516107276865593max eps sqaure: tensor([39.0872], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2516107276865593max eps sqaure: tensor([39.0872], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2516107276865593max eps sqaure: tensor([39.0872], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2516107276865593max eps sqaure: tensor([39.0872], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2516107276865593max eps sqaure: tensor([39.0872], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2516107276865593max eps sqaure: tensor([39.0872], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2516107276865593max eps sqaure: tensor([39.0872], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2516107276865593max eps sqaure: tensor([39.0872], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2516107276865593max eps sqaure: tensor([39.0872], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2516107276865593max eps sqaure: tensor([39.0872], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2516107276865593max eps sqaure: tensor([39.0872], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2516107276865593max eps sqaure: tensor([39.0872], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increase---> Checking nodes...10/ 1600.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 28446.818877571688regular term: 2821.6211196971367Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 2499.16304056946max eps sqaure: tensor([3651.1074], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2380.1009991395094max eps sqaure: tensor([3436.8150], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2243.1165461176993max eps sqaure: tensor([3189.2758], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2086.7607579988585max eps sqaure: tensor([2905.7772], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1910.1766350430844max eps sqaure: tensor([2582.1940], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1713.4288644561718max eps sqaure: tensor([2215.3638], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1498.77500764009max eps sqaure: tensor([1956.2057], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1271.981998458782max eps sqaure: tensor([1802.5178], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1045.977038792327max eps sqaure: tensor([1716.3055], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 840.5392619486393max eps sqaure: tensor([1608.8113], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 673.6703229143691max eps sqaure: tensor([1451.7836], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 536.2123472463239max eps sqaure: tensor([1208.4331], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 396.3718193643069max eps sqaure: tensor([872.2571], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 245.9142895375925max eps sqaure: tensor([496.7530], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 112.3341747256667max eps sqaure: tensor([239.2493], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 32.993892916757595max eps sqaure: tensor([152.4803], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 14.84682834059362max eps sqaure: tensor([147.9895], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 13.80511971815121max eps sqaure: tensor([84.3193], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 9.614229361595514max eps sqaure: tensor([67.0450], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 8.314675926038069max eps sqaure: tensor([55.3054], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 7.242457366185128max eps sqaure: tensor([45.4109], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.673701290675748max eps sqaure: tensor([38.3003], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.933877617565074max eps sqaure: tensor([31.1166], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.424809161518728max eps sqaure: tensor([29.6538], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.2682479458609635max eps sqaure: tensor([47.7769], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.978002392133415max eps sqaure: tensor([69.9621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.209154384917988max eps sqaure: tensor([92.4742], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 4.6537099000471756max eps sqaure: tensor([69.9621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.6537099000471756max eps sqaure: tensor([69.9621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.6537099000471756max eps sqaure: tensor([69.9621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.6537099000471756max eps sqaure: tensor([69.9621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.6537099000471756max eps sqaure: tensor([69.9621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.6537099000471756max eps sqaure: tensor([69.9621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.6537099000471756max eps sqaure: tensor([69.9621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.6537099000471756max eps sqaure: tensor([69.9621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.6537099000471756max eps sqaure: tensor([69.9621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.6537099000471756max eps sqaure: tensor([69.9621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.6537099000471756max eps sqaure: tensor([69.9621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.6537099000471756max eps sqaure: tensor([69.9621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.6537099000471756max eps sqaure: tensor([69.9621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.6537099000471756max eps sqaure: tensor([69.9621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.6537099000471756max eps sqaure: tensor([69.9621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.6537099000471756max eps sqaure: tensor([69.9621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.6537099000471756max eps sqaure: tensor([69.9621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.6537099000471756max eps sqaure: tensor([69.9621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.6537099000471756max eps sqaure: tensor([69.9621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.6537099000471756max eps sqaure: tensor([69.9621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.6537099000471756max eps sqaure: tensor([69.9621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.6537099000471756max eps sqaure: tensor([69.9621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.6537099000471756max eps sqaure: tensor([69.9621], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increase---> Checking nodes...11/ 1600.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 28446.818877571688regular term: 2821.6211196971367Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 3106.0719060210217max eps sqaure: tensor([5178.1597], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2968.271760311823max eps sqaure: tensor([4713.1238], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2809.9120762095404max eps sqaure: tensor([4224.4107], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2629.336711668991max eps sqaure: tensor([3729.7282], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2425.4646088474055max eps sqaure: tensor([3364.1778], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2197.7176857357363max eps sqaure: tensor([2960.4641], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1946.8576374992456max eps sqaure: tensor([2512.2982], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1676.1033193182923max eps sqaure: tensor([2110.8963], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1394.0654189005074max eps sqaure: tensor([1857.7020], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1119.153892015396max eps sqaure: tensor([1661.7709], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 879.0342497277202max eps sqaure: tensor([1513.2335], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 689.8222778547553max eps sqaure: tensor([1305.0562], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 524.4073767990409max eps sqaure: tensor([1006.5278], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 349.3183616463337max eps sqaure: tensor([648.2020], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 182.99914193563598max eps sqaure: tensor([337.5989], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 68.36318774591895max eps sqaure: tensor([210.4270], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 22.410233311239722max eps sqaure: tensor([124.5662], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 16.243234464608246max eps sqaure: tensor([114.2301], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 13.450172704211282max eps sqaure: tensor([78.1875], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 10.192521367484082max eps sqaure: tensor([66.0612], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 8.89168937431875max eps sqaure: tensor([55.8291], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 7.753354151306581max eps sqaure: tensor([48.2147], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 7.130039952761325max eps sqaure: tensor([42.0581], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.693620395350481max eps sqaure: tensor([33.2326], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.344631157260072max eps sqaure: tensor([30.2143], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.05337640224202max eps sqaure: tensor([29.5264], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.98008542672854max eps sqaure: tensor([38.6485], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.789113162553893max eps sqaure: tensor([83.7461], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.233008391514214max eps sqaure: tensor([83.2241], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 6.8508725408880435max eps sqaure: tensor([83.7461], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 6.8508725408880435max eps sqaure: tensor([83.7461], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 6.8508725408880435max eps sqaure: tensor([83.7461], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 6.8508725408880435max eps sqaure: tensor([83.7461], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 6.8508725408880435max eps sqaure: tensor([83.7461], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 6.8508725408880435max eps sqaure: tensor([83.7461], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 6.8508725408880435max eps sqaure: tensor([83.7461], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 6.8508725408880435max eps sqaure: tensor([83.7461], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 6.8508725408880435max eps sqaure: tensor([83.7461], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 6.8508725408880435max eps sqaure: tensor([83.7461], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 6.8508725408880435max eps sqaure: tensor([83.7461], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 6.8508725408880435max eps sqaure: tensor([83.7461], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 6.8508725408880435max eps sqaure: tensor([83.7461], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 6.8508725408880435max eps sqaure: tensor([83.7461], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 6.8508725408880435max eps sqaure: tensor([83.7461], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 6.8508725408880435max eps sqaure: tensor([83.7461], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 6.8508725408880435max eps sqaure: tensor([83.7461], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 6.8508725408880435max eps sqaure: tensor([83.7461], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 6.8508725408880435max eps sqaure: tensor([83.7461], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 6.8508725408880435max eps sqaure: tensor([83.7461], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 6.8508725408880435max eps sqaure: tensor([83.7461], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decrease---> Checking nodes...12/ 1600.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 28446.818877571688regular term: 2821.6211196971367Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 2498.885483199491max eps sqaure: tensor([3697.1228], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2378.5439662015483max eps sqaure: tensor([3469.1698], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2240.3053715056535max eps sqaure: tensor([3222.4580], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2082.8788992261484max eps sqaure: tensor([2938.4749], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1905.6516818466553max eps sqaure: tensor([2614.2057], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1709.199911101549max eps sqaure: tensor([2261.8405], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1495.9118152705407max eps sqaure: tensor([2063.5434], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1272.3584035471438max eps sqaure: tensor([1961.7121], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1051.4460749750117max eps sqaure: tensor([1898.7543], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 852.318350784302max eps sqaure: tensor([1810.8085], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 690.3788800933004max eps sqaure: tensor([1676.3927], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 557.7977510520534max eps sqaure: tensor([1454.2381], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 428.29751858274676max eps sqaure: tensor([1131.9482], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 288.4523517711595max eps sqaure: tensor([740.2962], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 149.94542508921324max eps sqaure: tensor([361.1045], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 52.84779293248181max eps sqaure: tensor([226.3022], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 18.334351891449565max eps sqaure: tensor([136.7030], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 15.831825871716582max eps sqaure: tensor([110.2683], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 12.077131319227457max eps sqaure: tensor([74.6873], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 9.11644210451726max eps sqaure: tensor([78.3810], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 7.471393474086039max eps sqaure: tensor([58.8768], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.39399788810539max eps sqaure: tensor([46.1507], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.52917451337282max eps sqaure: tensor([38.6147], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.589158871163085max eps sqaure: tensor([29.9760], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.888061623097586max eps sqaure: tensor([26.8292], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.7096684173154197max eps sqaure: tensor([47.9746], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.7413735669007693max eps sqaure: tensor([71.8654], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 2.8063207399735055max eps sqaure: tensor([47.9746], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8063207399735055max eps sqaure: tensor([47.9746], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8063207399735055max eps sqaure: tensor([47.9746], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8063207399735055max eps sqaure: tensor([47.9746], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8063207399735055max eps sqaure: tensor([47.9746], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8063207399735055max eps sqaure: tensor([47.9746], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8063207399735055max eps sqaure: tensor([47.9746], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8063207399735055max eps sqaure: tensor([47.9746], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8063207399735055max eps sqaure: tensor([47.9746], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8063207399735055max eps sqaure: tensor([47.9746], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8063207399735055max eps sqaure: tensor([47.9746], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8063207399735055max eps sqaure: tensor([47.9746], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8063207399735055max eps sqaure: tensor([47.9746], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8063207399735055max eps sqaure: tensor([47.9746], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8063207399735055max eps sqaure: tensor([47.9746], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8063207399735055max eps sqaure: tensor([47.9746], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8063207399735055max eps sqaure: tensor([47.9746], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8063207399735055max eps sqaure: tensor([47.9746], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8063207399735055max eps sqaure: tensor([47.9746], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8063207399735055max eps sqaure: tensor([47.9746], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8063207399735055max eps sqaure: tensor([47.9746], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8063207399735055max eps sqaure: tensor([47.9746], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8063207399735055max eps sqaure: tensor([47.9746], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increase---> Checking nodes...13/ 1600.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 28446.818877571688regular term: 2821.6211196971367Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 2819.4373092005553max eps sqaure: tensor([4463.6412], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2696.8902919312113max eps sqaure: tensor([4050.5324], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2555.313375713232max eps sqaure: tensor([3710.5371], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2392.839690631478max eps sqaure: tensor([3420.9251], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2208.1007015290015max eps sqaure: tensor([3087.9544], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2000.4198337861471max eps sqaure: tensor([2708.9141], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1770.4635205307125max eps sqaure: tensor([2320.2923], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1522.2302886751697max eps sqaure: tensor([2106.2053], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1265.8697837210636max eps sqaure: tensor([1973.2052], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1021.6106010646699max eps sqaure: tensor([1905.1493], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 817.4160005488103max eps sqaure: tensor([1808.9332], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 665.9141001640425max eps sqaure: tensor([1658.7419], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 538.9662008662833max eps sqaure: tensor([1413.7775], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 400.58475212070965max eps sqaure: tensor([1037.9506], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 250.1194649600434max eps sqaure: tensor([601.2908], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 111.93253896851171max eps sqaure: tensor([270.9259], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 30.89566981132133max eps sqaure: tensor([201.3557], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 17.704005530136364max eps sqaure: tensor([133.8052], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 15.818958764762323max eps sqaure: tensor([103.3939], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 11.599637878341808max eps sqaure: tensor([100.1642], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 9.532832940664262max eps sqaure: tensor([70.0347], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 8.212607466739689max eps sqaure: tensor([55.9439], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 7.059263530205068max eps sqaure: tensor([40.8988], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.792353953801448max eps sqaure: tensor([30.5456], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.833993701989986max eps sqaure: tensor([24.1554], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.080479641546485max eps sqaure: tensor([32.8527], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.052400235347044max eps sqaure: tensor([62.6030], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.7687970910908963max eps sqaure: tensor([66.8103], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3819804120172474max eps sqaure: tensor([46.1674], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.9090475164491525max eps sqaure: tensor([44.3161], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.185356486730751max eps sqaure: tensor([42.7737], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.002770200236723max eps sqaure: tensor([44.3161], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.002770200236723max eps sqaure: tensor([44.3161], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.002770200236723max eps sqaure: tensor([44.3161], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.002770200236723max eps sqaure: tensor([44.3161], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.002770200236723max eps sqaure: tensor([44.3161], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.002770200236723max eps sqaure: tensor([44.3161], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.002770200236723max eps sqaure: tensor([44.3161], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.002770200236723max eps sqaure: tensor([44.3161], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.002770200236723max eps sqaure: tensor([44.3161], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.002770200236723max eps sqaure: tensor([44.3161], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.002770200236723max eps sqaure: tensor([44.3161], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.002770200236723max eps sqaure: tensor([44.3161], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.002770200236723max eps sqaure: tensor([44.3161], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.002770200236723max eps sqaure: tensor([44.3161], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.002770200236723max eps sqaure: tensor([44.3161], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.002770200236723max eps sqaure: tensor([44.3161], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.002770200236723max eps sqaure: tensor([44.3161], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.002770200236723max eps sqaure: tensor([44.3161], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.002770200236723max eps sqaure: tensor([44.3161], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decrease---> Checking nodes...14/ 1600.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 28446.818877571688regular term: 2821.6211196971367Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 1364.2518565626897max eps sqaure: tensor([1800.9480], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1284.8624615791034max eps sqaure: tensor([1684.5295], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1194.996158434419max eps sqaure: tensor([1595.0632], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1095.0037701718575max eps sqaure: tensor([1538.0306], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 986.3693345228947max eps sqaure: tensor([1493.3585], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 872.1792719604997max eps sqaure: tensor([1443.8159], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 757.2570073383157max eps sqaure: tensor([1383.9079], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 647.0901480054127max eps sqaure: tensor([1345.9767], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 545.150029980143max eps sqaure: tensor([1293.3538], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 445.80483542504135max eps sqaure: tensor([1185.0449], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 335.47430002155437max eps sqaure: tensor([918.6724], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 213.53260768318216max eps sqaure: tensor([504.9491], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 106.41329382309233max eps sqaure: tensor([257.8728], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 39.107615421103546max eps sqaure: tensor([152.9703], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 13.39150080352445max eps sqaure: tensor([90.2475], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 9.838911036679272max eps sqaure: tensor([64.8878], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 8.392339099027385max eps sqaure: tensor([55.1700], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.307508853426353max eps sqaure: tensor([46.6565], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.355124479076999max eps sqaure: tensor([38.7948], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.465742759000029max eps sqaure: tensor([31.5365], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.859317455862678max eps sqaure: tensor([27.5888], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.517186181898847max eps sqaure: tensor([21.8601], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3963019827678713max eps sqaure: tensor([21.2026], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.285926921002463max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.328089678332353max eps sqaure: tensor([26.0883], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 2.237819515977763max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.237819515977763max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.237819515977763max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.237819515977763max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.237819515977763max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.237819515977763max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.237819515977763max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.237819515977763max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.237819515977763max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.237819515977763max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.237819515977763max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.237819515977763max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.237819515977763max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.237819515977763max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.237819515977763max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.237819515977763max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.237819515977763max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.237819515977763max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.237819515977763max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.237819515977763max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.237819515977763max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.237819515977763max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.237819515977763max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.237819515977763max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.237819515977763max eps sqaure: tensor([19.5829], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increase---> Checking nodes...15/ 1600.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 28446.818877571688regular term: 2821.6211196971367Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Restore model and lr decreasetrain_loss: 644.9453142150048regular term: 2821.6211464816133Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 2639.2316721108546max eps sqaure: tensor([4084.2257], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2522.5694999818293max eps sqaure: tensor([3704.4439], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2388.4013349718475max eps sqaure: tensor([3427.4833], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2234.684330803539max eps sqaure: tensor([3155.8006], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2059.7312855287523max eps sqaure: tensor([2843.1204], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1862.4592402736796max eps sqaure: tensor([2484.6117], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1643.3786768968691max eps sqaure: tensor([2127.3249], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1406.3457967106121max eps sqaure: tensor([1921.5981], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1162.0602692158511max eps sqaure: tensor([1802.8584], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 931.5806103617983max eps sqaure: tensor([1723.1511], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 742.0695592928374max eps sqaure: tensor([1609.2331], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 600.0752945735367max eps sqaure: tensor([1423.8614], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 469.9637910018462max eps sqaure: tensor([1129.4415], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 325.26761145595293max eps sqaure: tensor([742.2521], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 177.02386840664235max eps sqaure: tensor([351.4895], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 60.45068761838824max eps sqaure: tensor([201.9671], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 17.649137154431052max eps sqaure: tensor([147.7723], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 16.556804007783366max eps sqaure: tensor([124.7287], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 12.04188124847222max eps sqaure: tensor([78.7723], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 9.568887035550217max eps sqaure: tensor([61.5312], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 8.217584348545891max eps sqaure: tensor([50.4475], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 7.218599859995627max eps sqaure: tensor([45.4268], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.396330437886488max eps sqaure: tensor([40.9174], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.645759811000327max eps sqaure: tensor([33.8325], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.196593022356144max eps sqaure: tensor([30.7233], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.938195460031703max eps sqaure: tensor([47.8588], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.6385463528028525max eps sqaure: tensor([86.9901], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.451619054906211max eps sqaure: tensor([65.0641], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.451091298380224max eps sqaure: tensor([41.7654], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.1089386317666077max eps sqaure: tensor([33.9642], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.736216873577128max eps sqaure: tensor([41.1009], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.1166009617786528max eps sqaure: tensor([33.9642], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.1166009617786528max eps sqaure: tensor([33.9642], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.1166009617786528max eps sqaure: tensor([33.9642], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.1166009617786528max eps sqaure: tensor([33.9642], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.1166009617786528max eps sqaure: tensor([33.9642], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.1166009617786528max eps sqaure: tensor([33.9642], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.1166009617786528max eps sqaure: tensor([33.9642], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.1166009617786528max eps sqaure: tensor([33.9642], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.1166009617786528max eps sqaure: tensor([33.9642], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.1166009617786528max eps sqaure: tensor([33.9642], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.1166009617786528max eps sqaure: tensor([33.9642], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.1166009617786528max eps sqaure: tensor([33.9642], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.1166009617786528max eps sqaure: tensor([33.9642], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.1166009617786528max eps sqaure: tensor([33.9642], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.1166009617786528max eps sqaure: tensor([33.9642], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.1166009617786528max eps sqaure: tensor([33.9642], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.1166009617786528max eps sqaure: tensor([33.9642], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.1166009617786528max eps sqaure: tensor([33.9642], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.1166009617786528max eps sqaure: tensor([33.9642], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasemodel after reorg: TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)


### New lts ###Total obtaining n: 333
obtaining n over lr goal: 0
Total select n: 338
select n over lr goal: 5
//////////// Start module_EU_LG_UA Epoch ///////////Finish epoch. non acceptable module at max eps tensor([116220.1548], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)////////// Start CRAM and REORG with unacceptable wt //////////model after wt: TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)eps_square (last 10) before cram: tensor([6.9044, 7.2054, 7.2848, 7.3423, 7.3833, 7.7633, 7.7757, 8.0656, 8.0983,
        8.2367], device='cuda:0', dtype=torch.float64, grad_fn=<ViewBackward0>)cramming sample 333th | total of 5cramming sample 334th | total of 5cramming sample 335th | total of 5cramming sample 336th | total of 5cramming sample 337th | total of 5model after cram: TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=31, bias=True)
  (layer_out): Linear(in_features=31, out_features=1, bias=True)
  (relu): ReLU()
)eps_sqaure (last 10) after cram: tensor([6.9044e+00, 7.2054e+00, 7.2848e+00, 7.3423e+00, 7.3833e+00, 2.9323e-24,
        5.2940e-23, 1.2287e-24, 1.6634e-23, 3.7340e-25], device='cuda:0',
       dtype=torch.float64, grad_fn=<ViewBackward0>)Acceptable SLFN exist in 'acceptable/cram.pth'=================== reorganising ===================---> Checking nodes...0/ 3100.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=31, bias=True)
  (layer_out): Linear(in_features=31, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 103158.38124962593regular term: 1505.4311240503096Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 2402.219355890957max eps sqaure: tensor([3471.7849], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2258.3935558400785max eps sqaure: tensor([3111.8099], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2094.9892439265886max eps sqaure: tensor([2850.9897], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1911.0897638298222max eps sqaure: tensor([2551.0472], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1706.6344867273094max eps sqaure: tensor([2231.2383], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1484.2285584848594max eps sqaure: tensor([2063.4573], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1252.4651166964122max eps sqaure: tensor([1951.3116], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1030.0232673714688max eps sqaure: tensor([1915.6369], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 843.9060340354707max eps sqaure: tensor([1858.7502], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 705.2792557907534max eps sqaure: tensor([1746.1467], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 587.5818072252482max eps sqaure: tensor([1541.3023], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 459.9408391153663max eps sqaure: tensor([1218.9990], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 316.0188810897864max eps sqaure: tensor([810.4470], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 173.06022464147256max eps sqaure: tensor([411.6981], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 70.30474580239421max eps sqaure: tensor([281.1917], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 28.398805753674363max eps sqaure: tensor([179.1952], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 21.133242028771985max eps sqaure: tensor([130.4101], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 15.523043916958754max eps sqaure: tensor([93.8171], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 11.237588355647667max eps sqaure: tensor([71.9805], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 8.846007277369447max eps sqaure: tensor([51.9814], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 7.221033093531094max eps sqaure: tensor([40.6464], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.075028625592934max eps sqaure: tensor([38.6731], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.198832944899613max eps sqaure: tensor([34.3325], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.513083759024966max eps sqaure: tensor([33.5937], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.970853618816896max eps sqaure: tensor([30.7019], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.5981888449001147max eps sqaure: tensor([27.0454], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.4912436124997375max eps sqaure: tensor([23.5246], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.410035190205091max eps sqaure: tensor([19.2497], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.7964026322818207max eps sqaure: tensor([19.1370], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 2.8244782964962614max eps sqaure: tensor([19.2497], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8244782964962614max eps sqaure: tensor([19.2497], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8244782964962614max eps sqaure: tensor([19.2497], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8244782964962614max eps sqaure: tensor([19.2497], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8244782964962614max eps sqaure: tensor([19.2497], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8244782964962614max eps sqaure: tensor([19.2497], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8244782964962614max eps sqaure: tensor([19.2497], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8244782964962614max eps sqaure: tensor([19.2497], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8244782964962614max eps sqaure: tensor([19.2497], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8244782964962614max eps sqaure: tensor([19.2497], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8244782964962614max eps sqaure: tensor([19.2497], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8244782964962614max eps sqaure: tensor([19.2497], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8244782964962614max eps sqaure: tensor([19.2497], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8244782964962614max eps sqaure: tensor([19.2497], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8244782964962614max eps sqaure: tensor([19.2497], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8244782964962614max eps sqaure: tensor([19.2497], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8244782964962614max eps sqaure: tensor([19.2497], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8244782964962614max eps sqaure: tensor([19.2497], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8244782964962614max eps sqaure: tensor([19.2497], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8244782964962614max eps sqaure: tensor([19.2497], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8244782964962614max eps sqaure: tensor([19.2497], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increase---> Checking nodes...1/ 3100.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=31, bias=True)
  (layer_out): Linear(in_features=31, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 103158.38124962593regular term: 1505.4311240503096Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 2540.671751321219max eps sqaure: tensor([3683.2961], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2382.247525402132max eps sqaure: tensor([3369.6200], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2203.114941682392max eps sqaure: tensor([3063.1626], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2003.483819684928max eps sqaure: tensor([2718.5007], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1784.6750069114514max eps sqaure: tensor([2332.0185], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1550.1588536985666max eps sqaure: tensor([2108.1976], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1308.9005271189221max eps sqaure: tensor([1937.3493], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1077.8751851574555max eps sqaure: tensor([1879.7841], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 881.5571468971993max eps sqaure: tensor([1805.5458], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 731.1719250133051max eps sqaure: tensor([1679.2706], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 603.6886835323536max eps sqaure: tensor([1478.3520], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 469.17664547936425max eps sqaure: tensor([1156.6173], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 318.43184608187096max eps sqaure: tensor([774.9615], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 172.49716247475794max eps sqaure: tensor([400.9545], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 67.06882504857512max eps sqaure: tensor([209.6652], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 23.973464812398092max eps sqaure: tensor([147.4801], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 17.868216283509394max eps sqaure: tensor([101.2508], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 13.511709726539076max eps sqaure: tensor([69.4914], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 9.683468590093034max eps sqaure: tensor([60.8336], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 7.513091627415613max eps sqaure: tensor([44.3813], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.927038389305809max eps sqaure: tensor([32.5636], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.8481415131494545max eps sqaure: tensor([28.4349], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.0751222262566875max eps sqaure: tensor([26.3997], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.4662773342077813max eps sqaure: tensor([28.8466], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3934021547671587max eps sqaure: tensor([24.0366], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.0445336794977127max eps sqaure: tensor([26.2104], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.6437510723555744max eps sqaure: tensor([16.5876], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 2.7157031645039633max eps sqaure: tensor([26.2104], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.7157031645039633max eps sqaure: tensor([26.2104], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.7157031645039633max eps sqaure: tensor([26.2104], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.7157031645039633max eps sqaure: tensor([26.2104], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.7157031645039633max eps sqaure: tensor([26.2104], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.7157031645039633max eps sqaure: tensor([26.2104], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.7157031645039633max eps sqaure: tensor([26.2104], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.7157031645039633max eps sqaure: tensor([26.2104], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.7157031645039633max eps sqaure: tensor([26.2104], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.7157031645039633max eps sqaure: tensor([26.2104], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.7157031645039633max eps sqaure: tensor([26.2104], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.7157031645039633max eps sqaure: tensor([26.2104], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.7157031645039633max eps sqaure: tensor([26.2104], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.7157031645039633max eps sqaure: tensor([26.2104], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.7157031645039633max eps sqaure: tensor([26.2104], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.7157031645039633max eps sqaure: tensor([26.2104], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.7157031645039633max eps sqaure: tensor([26.2104], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.7157031645039633max eps sqaure: tensor([26.2104], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.7157031645039633max eps sqaure: tensor([26.2104], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.7157031645039633max eps sqaure: tensor([26.2104], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.7157031645039633max eps sqaure: tensor([26.2104], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.7157031645039633max eps sqaure: tensor([26.2104], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.7157031645039633max eps sqaure: tensor([26.2104], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increase---> Checking nodes...2/ 3100.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=31, bias=True)
  (layer_out): Linear(in_features=31, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 103158.38124962593regular term: 1505.4311240503096Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 2463.3140243517014max eps sqaure: tensor([3482.7179], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2308.142930234243max eps sqaure: tensor([3224.9454], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2132.928870964596max eps sqaure: tensor([2932.1709], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1938.0326249369155max eps sqaure: tensor([2603.8447], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1724.7873958609853max eps sqaure: tensor([2236.5878], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1496.9528335239454max eps sqaure: tensor([2050.1317], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1264.1197068805159max eps sqaure: tensor([1888.2777], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1044.2517343001207max eps sqaure: tensor([1802.9303], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 860.3381601369575max eps sqaure: tensor([1696.6541], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 715.126018908219max eps sqaure: tensor([1549.9523], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 583.292787753571max eps sqaure: tensor([1333.5000], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 445.5942865189299max eps sqaure: tensor([1030.3505], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 298.4869438815324max eps sqaure: tensor([651.3727], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 154.57641872184973max eps sqaure: tensor([346.1399], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 57.73157574274853max eps sqaure: tensor([241.1059], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 24.60565362379643max eps sqaure: tensor([148.6783], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 19.692173558113392max eps sqaure: tensor([109.1396], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 14.071691383332405max eps sqaure: tensor([86.2504], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 10.308163337335136max eps sqaure: tensor([61.8782], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 7.905451414642381max eps sqaure: tensor([42.7563], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.336196648247397max eps sqaure: tensor([33.9215], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.317145221431553max eps sqaure: tensor([31.1639], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.56495549166667max eps sqaure: tensor([29.3543], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.046487024117526max eps sqaure: tensor([27.7310], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.6684104189088376max eps sqaure: tensor([26.4742], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3830384997651115max eps sqaure: tensor([25.7097], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.265913565972456max eps sqaure: tensor([23.1668], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.269794013476163max eps sqaure: tensor([18.3422], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 3.0229739609909227max eps sqaure: tensor([23.1668], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.0229739609909227max eps sqaure: tensor([23.1668], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.0229739609909227max eps sqaure: tensor([23.1668], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.0229739609909227max eps sqaure: tensor([23.1668], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.0229739609909227max eps sqaure: tensor([23.1668], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.0229739609909227max eps sqaure: tensor([23.1668], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.0229739609909227max eps sqaure: tensor([23.1668], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.0229739609909227max eps sqaure: tensor([23.1668], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.0229739609909227max eps sqaure: tensor([23.1668], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.0229739609909227max eps sqaure: tensor([23.1668], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.0229739609909227max eps sqaure: tensor([23.1668], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.0229739609909227max eps sqaure: tensor([23.1668], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.0229739609909227max eps sqaure: tensor([23.1668], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.0229739609909227max eps sqaure: tensor([23.1668], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.0229739609909227max eps sqaure: tensor([23.1668], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.0229739609909227max eps sqaure: tensor([23.1668], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.0229739609909227max eps sqaure: tensor([23.1668], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.0229739609909227max eps sqaure: tensor([23.1668], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.0229739609909227max eps sqaure: tensor([23.1668], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.0229739609909227max eps sqaure: tensor([23.1668], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.0229739609909227max eps sqaure: tensor([23.1668], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.0229739609909227max eps sqaure: tensor([23.1668], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increase---> Checking nodes...3/ 3100.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=31, bias=True)
  (layer_out): Linear(in_features=31, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 103158.38124962593regular term: 1505.4311240503096Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Restore model and lr decreasetrain_loss: 559.2371650298707regular term: 1505.4311782122154Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 2703.32830817003max eps sqaure: tensor([3937.8687], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2538.993516798155max eps sqaure: tensor([3580.1547], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2353.24338392937max eps sqaure: tensor([3266.9471], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2146.0952428796572max eps sqaure: tensor([2915.7540], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1918.54955399198max eps sqaure: tensor([2522.5936], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1673.7189764468055max eps sqaure: tensor([2278.7203], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1419.362186871912max eps sqaure: tensor([2068.9374], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1170.8437102294413max eps sqaure: tensor([1956.6828], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 953.7189565485575max eps sqaure: tensor([1863.6595], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 786.2792856788739max eps sqaure: tensor([1736.6982], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 651.6347429952805max eps sqaure: tensor([1552.1694], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 517.3071425110332max eps sqaure: tensor([1276.0935], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 372.9052087060343max eps sqaure: tensor([905.0420], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 224.85442832571178max eps sqaure: tensor([505.9229], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 99.60119455931103max eps sqaure: tensor([294.1462], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 34.84075217347945max eps sqaure: tensor([195.7234], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 20.77953026252762max eps sqaure: tensor([124.7973], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 16.369275096007954max eps sqaure: tensor([94.5239], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 11.28799418286671max eps sqaure: tensor([69.6926], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 8.556705679685853max eps sqaure: tensor([53.2954], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.701491974089247max eps sqaure: tensor([42.9925], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.584377009576012max eps sqaure: tensor([34.6062], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.7522693886319844#######################################2024-01-06 17:12:04.524072#######################################


=======================//////////// Start New ltsTotal obtaining n: 328
obtaining n over lr goal: 0
Total select n: 333
select n over lr goal: 5
//////////// Start module_EU_LG_UA EpochFinish epoch. non acceptable module at max eps tensor([9.8415], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)=======================////////// Start CRAM and REORG with unacceptable wt //////////model after wt: TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=1, bias=True)
  (layer_out): Linear(in_features=1, out_features=1, bias=True)
  (relu): ReLU()
)eps_square (last 10) before cram: tensor([6.9044, 7.2054, 7.2848, 7.3423, 7.3833, 7.5202, 7.6095, 7.6147, 7.6644,
        7.6826], device='cuda:0', dtype=torch.float64, grad_fn=<ViewBackward0>)cramming sample 328th | total of 5cramming sample 329th | total of 5cramming sample 330th | total of 5cramming sample 331th | total of 5cramming sample 332th | total of 5model after cram: TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)eps_sqaure (last 10) after cram: tensor([6.9044e+00, 7.2054e+00, 7.2848e+00, 7.3423e+00, 7.3833e+00, 1.2445e-22,
        3.7340e-23, 5.4607e-25, 1.1059e-24, 4.1717e-23], device='cuda:0',
       dtype=torch.float64, grad_fn=<ViewBackward0>)Acceptable SLFN exist in 'acceptable/cram.pth'-----> reorganising---> Checking nodes...0/ 1600.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 20049.65708765744regular term: 2821.6217744291794Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 1642.89316647742max eps sqaure: tensor([2457.5122], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1550.7456364306697max eps sqaure: tensor([2292.8686], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1445.6046554997884max eps sqaure: tensor([2100.9863], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1327.235527165317max eps sqaure: tensor([1880.1131], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1197.0948581211585max eps sqaure: tensor([1662.7447], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1058.7574008912222max eps sqaure: tensor([1622.8067], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 918.4938815422539max eps sqaure: tensor([1581.8538], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 786.2386605464577max eps sqaure: tensor([1533.7867], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 671.3577150829755max eps sqaure: tensor([1458.4155], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 576.0704078054083max eps sqaure: tensor([1404.8741], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 484.2377914038988max eps sqaure: tensor([1264.5788], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 365.8166235894983max eps sqaure: tensor([882.7801], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 230.28770202112403max eps sqaure: tensor([569.1833], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 112.58849321580453max eps sqaure: tensor([359.7802], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 40.29716716681131max eps sqaure: tensor([271.9033], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 16.194043550368935max eps sqaure: tensor([173.6257], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 13.559187544293609max eps sqaure: tensor([129.6819], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 10.37659595376786max eps sqaure: tensor([104.6742], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 7.819178108721329max eps sqaure: tensor([78.8166], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.327536416512761max eps sqaure: tensor([58.4775], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.167762331182643max eps sqaure: tensor([43.2986], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.27947518277536max eps sqaure: tensor([34.2641], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.588170288426719max eps sqaure: tensor([28.1489], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.0990049462738862max eps sqaure: tensor([22.5248], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.8201951851602max eps sqaure: tensor([31.0434], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.5225340925877715max eps sqaure: tensor([37.7785], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.4358277040361376max eps sqaure: tensor([30.8473], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.3194967928982573max eps sqaure: tensor([35.0905], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.370328282809808max eps sqaure: tensor([37.6193], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 2.0200148331243337max eps sqaure: tensor([35.0905], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.0200148331243337max eps sqaure: tensor([35.0905], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.0200148331243337max eps sqaure: tensor([35.0905], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.0200148331243337max eps sqaure: tensor([35.0905], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.0200148331243337max eps sqaure: tensor([35.0905], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.0200148331243337max eps sqaure: tensor([35.0905], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.0200148331243337max eps sqaure: tensor([35.0905], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.0200148331243337max eps sqaure: tensor([35.0905], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.0200148331243337max eps sqaure: tensor([35.0905], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.0200148331243337max eps sqaure: tensor([35.0905], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.0200148331243337max eps sqaure: tensor([35.0905], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.0200148331243337max eps sqaure: tensor([35.0905], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.0200148331243337max eps sqaure: tensor([35.0905], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.0200148331243337max eps sqaure: tensor([35.0905], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.0200148331243337max eps sqaure: tensor([35.0905], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.0200148331243337max eps sqaure: tensor([35.0905], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.0200148331243337max eps sqaure: tensor([35.0905], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.0200148331243337max eps sqaure: tensor([35.0905], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.0200148331243337max eps sqaure: tensor([35.0905], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.0200148331243337max eps sqaure: tensor([35.0905], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.0200148331243337max eps sqaure: tensor([35.0905], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increase---> Checking nodes...1/ 1600.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 20049.65708765744regular term: 2821.6217744291794Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 2356.258210017407max eps sqaure: tensor([3440.6964], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2238.647467127334max eps sqaure: tensor([3246.3404], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2103.463370495704max eps sqaure: tensor([3020.6904], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1949.038778430935max eps sqaure: tensor([2758.9641], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1774.6231523243512max eps sqaure: tensor([2457.1263], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1580.4278619759273max eps sqaure: tensor([2111.7954], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1368.898063360306max eps sqaure: tensor([1807.0478], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1148.4003429379661max eps sqaure: tensor([1741.8160], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 936.9272766424227max eps sqaure: tensor([1687.0879], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 760.4680913500847max eps sqaure: tensor([1621.8480], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 633.9682666908006max eps sqaure: tensor([1500.5692], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 533.9826945072806max eps sqaure: tensor([1317.0159], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 423.02482376332273max eps sqaure: tensor([1055.0504], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 288.81181404933665max eps sqaure: tensor([695.8294], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 140.38846915010294max eps sqaure: tensor([337.7089], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 38.86131241429461max eps sqaure: tensor([232.7536], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 14.992845656140737max eps sqaure: tensor([123.5718], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 14.146829874728164max eps sqaure: tensor([107.6182], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 8.845934210547982max eps sqaure: tensor([91.1464], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 7.174527083760026max eps sqaure: tensor([65.2664], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.826251801695989max eps sqaure: tensor([45.6110], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.195997954902018max eps sqaure: tensor([36.9695], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.456163383666982max eps sqaure: tensor([34.9511], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.829376190923551max eps sqaure: tensor([40.5486], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.4573415503451943max eps sqaure: tensor([44.3354], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.2221791409568152max eps sqaure: tensor([52.1603], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.1538663816989394max eps sqaure: tensor([47.2466], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.800434218059133max eps sqaure: tensor([31.1875], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.5320017276118194max eps sqaure: tensor([42.2876], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.973229890045483max eps sqaure: tensor([40.4328], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 2.2559796515056556max eps sqaure: tensor([42.2876], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2559796515056556max eps sqaure: tensor([42.2876], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2559796515056556max eps sqaure: tensor([42.2876], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2559796515056556max eps sqaure: tensor([42.2876], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2559796515056556max eps sqaure: tensor([42.2876], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2559796515056556max eps sqaure: tensor([42.2876], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2559796515056556max eps sqaure: tensor([42.2876], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2559796515056556max eps sqaure: tensor([42.2876], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2559796515056556max eps sqaure: tensor([42.2876], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2559796515056556max eps sqaure: tensor([42.2876], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2559796515056556max eps sqaure: tensor([42.2876], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2559796515056556max eps sqaure: tensor([42.2876], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2559796515056556max eps sqaure: tensor([42.2876], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2559796515056556max eps sqaure: tensor([42.2876], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2559796515056556max eps sqaure: tensor([42.2876], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2559796515056556max eps sqaure: tensor([42.2876], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2559796515056556max eps sqaure: tensor([42.2876], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2559796515056556max eps sqaure: tensor([42.2876], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2559796515056556max eps sqaure: tensor([42.2876], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2559796515056556max eps sqaure: tensor([42.2876], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increase---> Checking nodes...2/ 1600.00TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 20049.65708765744regular term: 2821.6217744291794Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Restore model and lr decreasetrain_loss: 644.945460841172regular term: 2821.621789380966Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 1875.0120550729355max eps sqaure: tensor([2620.3273], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1769.4995837769186max eps sqaure: tensor([2441.9236], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1649.2154300768696max eps sqaure: tensor([2235.0754], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1513.8098991961112max eps sqaure: tensor([1998.3246], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1364.4365661808881max eps sqaure: tensor([1732.7549], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1203.7776628571885max eps sqaure: tensor([1622.5083], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1037.0345837919983max eps sqaure: tensor([1559.7485], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 872.5827982275626max eps sqaure: tensor([1495.1530], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 721.075004080846max eps sqaure: tensor([1448.8591], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 593.576219653561max eps sqaure: tensor([1370.1590], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 491.16799193570364max eps sqaure: tensor([1234.1483], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 398.19272720018466max eps sqaure: tensor([1085.5214], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 290.0730266469069max eps sqaure: tensor([758.6306], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 168.87419436682202max eps sqaure: tensor([404.1222], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 71.57660551310435max eps sqaure: tensor([227.2630], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 22.62474040924188max eps sqaure: tensor([143.6783], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 11.04966603898174max eps sqaure: tensor([88.2044], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 9.797765366688505max eps sqaure: tensor([64.3416], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 7.036278884666235max eps sqaure: tensor([56.6483], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.6525476453321595max eps sqaure: tensor([43.7658], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.719652373501643max eps sqaure: tensor([34.3463], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.9606220960915928max eps sqaure: tensor([26.9823], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.336270270866871max eps sqaure: tensor([23.8777], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.7605359313933904max eps sqaure: tensor([21.5372], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.4205478559117974max eps sqaure: tensor([19.3383], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.3244243723985702max eps sqaure: tensor([18.9887], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.331817864189334max eps sqaure: tensor([30.0376], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 1.9287143247935166max eps sqaure: tensor([18.9887], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1.9287143247935166max eps sqaure: tensor([18.9887], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1.9287143247935166max eps sqaure: tensor([18.9887], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1.9287143247935166max eps sqaure: tensor([18.9887], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1.9287143247935166max eps sqaure: tensor([18.9887], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1.9287143247935166max eps sqaure: tensor([18.9887], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increase#######################################2024-01-06 17:13:02.040157#######################################


=======================//////////// Start New ltsTotal obtaining n: 328
obtaining n over lr goal: 0
Total select n: 333
select n over lr goal: 5
//////////// Start module_EU_LG_UA EpochFinish epoch. non acceptable module at max eps tensor([9.8415], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)=======================////////// Start CRAM and REORG with unacceptable wt //////////model after wt: TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=1, bias=True)
  (layer_out): Linear(in_features=1, out_features=1, bias=True)
  (relu): ReLU()
)eps_square (last 10) before cram: tensor([6.9044, 7.2054, 7.2848, 7.3423, 7.3833, 7.5202, 7.6095, 7.6147, 7.6644,
        7.6826], device='cuda:0', dtype=torch.float64, grad_fn=<ViewBackward0>)cramming sample 328th | total of 5cramming sample 329th | total of 5cramming sample 330th | total of 5cramming sample 331th | total of 5cramming sample 332th | total of 5model after cram: TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)eps_sqaure (last 10) after cram: tensor([6.9044e+00, 7.2054e+00, 7.2848e+00, 7.3423e+00, 7.3833e+00, 4.2177e-23,
        1.4591e-22, 1.9080e-22, 3.8459e-24, 1.6003e-23], device='cuda:0',
       dtype=torch.float64, grad_fn=<ViewBackward0>)Acceptable SLFN exist in 'acceptable/cram.pth'-----> reorganising---> Checking nodes...0/16TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 16969.34844346433regular term: 2821.6187027414944Restore model and lr decreasetrain_loss: 644.9447631170768regular term: 2821.6187301290934Restore model and lr decreasetrain_loss: 644.9447631170768regular term: 2821.6187301290934Restore model and lr decreasetrain_loss: 644.9447631170768regular term: 2821.6187301290934Restore model and lr decreasetrain_loss: 644.9447631170768regular term: 2821.6187301290934Restore model and lr decreasetrain_loss: 644.9447631170768regular term: 2821.6187301290934Restore model and lr decreasetrain_loss: 644.9447631170768regular term: 2821.6187301290934Restore model and lr decreasetrain_loss: 644.9447631170768regular term: 2821.6187301290934Restore model and lr decreasetrain_loss: 644.9447631170768regular term: 2821.6187301290934Restore model and lr decreasetrain_loss: 644.9447631170768regular term: 2821.6187301290934Restore model and lr decreasetrain_loss: 644.9447631170768regular term: 2821.6187301290934Restore model and lr decreasetrain_loss: 644.9447631170768regular term: 2821.6187301290934Restore model and lr decreasetrain_loss: 644.9447631170768regular term: 2821.6187301290934Restore model and lr decreasetrain_loss: 644.9447631170768regular term: 2821.6187301290934Restore model and lr decreasetrain_loss: 644.9447631170768regular term: 2821.6187301290934Restore model and lr decreasetrain_loss: 644.9447631170768regular term: 2821.6187301290934Restore model and lr decreasetrain_loss: 644.9447631170768regular term: 2821.6187301290934Restore model and lr decreasetrain_loss: 644.9447631170768regular term: 2821.6187301290934Restore model and lr decreasetrain_loss: 644.9447631170768regular term: 2821.6187301290934Restore model and lr decreasetrain_loss: 644.9447631170768regular term: 2821.6187301290934Restore model and lr decreasetrain_loss: 644.9447631170768regular term: 2821.6187301290934Restore model and lr decreasetrain_loss: 644.9447631170768regular term: 2821.6187301290934Try trim model: Copy model and delete nodes success    --> Start module_EU_LG
train_loss: 2160.6748032022374max eps sqaure: tensor([3031.8765], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2068.221313556612max eps sqaure: tensor([2874.7873], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1961.4841110608186max eps sqaure: tensor([2690.1130], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1839.1693295796222max eps sqaure: tensor([2474.2740], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1700.134294500879max eps sqaure: tensor([2223.9315], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1543.6271818924758max eps sqaure: tensor([2033.9957], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1371.4998469362247max eps sqaure: tensor([1900.6335], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1189.6483231070456max eps sqaure: tensor([1894.6609], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 1010.0056900640972max eps sqaure: tensor([1899.0750], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 851.6098910692648max eps sqaure: tensor([1896.9862], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 731.5318345428228max eps sqaure: tensor([1851.0076], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 643.1569653730286max eps sqaure: tensor([1738.1744], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 556.9236816071497max eps sqaure: tensor([1625.6903], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 452.347353721963max eps sqaure: tensor([1338.5940], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 322.3165719009935max eps sqaure: tensor([877.0516], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 160.06887309201943max eps sqaure: tensor([322.5471], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 34.32802175653575max eps sqaure: tensor([212.4360], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 16.06838772562338max eps sqaure: tensor([135.8656], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 14.251355818111229max eps sqaure: tensor([118.8790], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 8.907697202936388max eps sqaure: tensor([94.4014], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 7.0501522014926605max eps sqaure: tensor([69.9500], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 6.1239000025118955max eps sqaure: tensor([50.7414], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 5.418692706678428max eps sqaure: tensor([41.4212], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.9312924423055655max eps sqaure: tensor([39.4181], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.748032514012874max eps sqaure: tensor([60.1939], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 4.3405748608702615max eps sqaure: tensor([63.9103], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.5800904733523673max eps sqaure: tensor([44.6816], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.0523913603486164max eps sqaure: tensor([39.8210], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.783867401695993max eps sqaure: tensor([43.0268], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 3.3912432015345844max eps sqaure: tensor([49.9826], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Restore model and lr decreasetrain_loss: 2.2677063701706284max eps sqaure: tensor([43.0268], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2677063701706284max eps sqaure: tensor([43.0268], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2677063701706284max eps sqaure: tensor([43.0268], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2677063701706284max eps sqaure: tensor([43.0268], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2677063701706284max eps sqaure: tensor([43.0268], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2677063701706284max eps sqaure: tensor([43.0268], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2677063701706284max eps sqaure: tensor([43.0268], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2677063701706284max eps sqaure: tensor([43.0268], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2677063701706284max eps sqaure: tensor([43.0268], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2677063701706284max eps sqaure: tensor([43.0268], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2677063701706284max eps sqaure: tensor([43.0268], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2677063701706284max eps sqaure: tensor([43.0268], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2677063701706284max eps sqaure: tensor([43.0268], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2677063701706284max eps sqaure: tensor([43.0268], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2677063701706284max eps sqaure: tensor([43.0268], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2677063701706284max eps sqaure: tensor([43.0268], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2677063701706284max eps sqaure: tensor([43.0268], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2677063701706284max eps sqaure: tensor([43.0268], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2677063701706284max eps sqaure: tensor([43.0268], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increasetrain_loss: 2.2677063701706284max eps sqaure: tensor([43.0268], device='cuda:0', dtype=torch.float64,
       grad_fn=<UnbindBackward0>)Save model and lr increase---> Checking nodes...1/16TwoLayerNet(
  (layer_1): Linear(in_features=18, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=1, bias=True)
  (relu): ReLU()
)    --> Start regularising_EU_LG_UAtrain_loss: 16969.34844346433regular term: 2821.6187027414944Restore model and lr decreasetrain_loss: 644.9447631170768regular term: 2821.6187301290934Restore model and lr decreasetrain_loss: 644.9447631170768regular term: 2821.6187301290934Restore model and lr decreasetrain_loss: 644.9447631170768regular term: 2821.6187301290934Restore model and lr decreasetrain_loss: 644.9447631170768regular term: 2821.6187301290934Restore model and lr decrease